{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidoc Minions\n",
    "\n",
    "Examples for running the minions (vanilla) on a multidoc dataset (list of markdown files).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/biderman/miniconda/envs/minions-py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:Secure crypto utilities not available. SecureClient will not function properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PEFT is not installed. Please install it with `pip install peft`.\n",
      "Warning: cartesia_mlx is not installed. If you want to use cartesia_mlx, please follow the instructions in the README to install it.\n",
      "Warning: huggingface inference client is not installed. If you want to use huggingface inference client, please install it with `pip install huggingface-hub`\n",
      "Warning: mlx_parallm is not installed. If you want to use mlx_parallm, please install it with `pip install mlx-parallm`\n",
      "Warning: NVIDIA attestation SDK not available: No module named 'nv_attestation_sdk'. GPU attestation will not work.\n",
      "Warning: cerebras-cloud-sdk is not installed. If you want to use CerebrasClient, please install it with `pip install cerebras-cloud-sdk`.\n",
      "chromadb is not installed. Please install it using `pip install chromadb`.\n"
     ]
    }
   ],
   "source": [
    "from local_rag_document_search import load_markdown_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading markdown files from: data/meeting_summaries\n",
      "Found 9 .md files\n",
      "--------------------------------------------------\n",
      "✓ Loaded: 1_q3_marketing_strategy_review.md (1872 chars)\n",
      "✓ Loaded: 2_1on1_performance_review_mike_rodriguez.md (1710 chars)\n",
      "✓ Loaded: 3_marketing_sales_alignment_meeting.md (1762 chars)\n",
      "✓ Loaded: 4_website_redesign_vendor_evaluation.md (1826 chars)\n",
      "✓ Loaded: 5_1on1_checkin_lisa_wang.md (1560 chars)\n",
      "✓ Loaded: 6_executive_leadership_team_monthly_update.md (1875 chars)\n",
      "✓ Loaded: 7_crisis_communication_planning_session.md (2028 chars)\n",
      "✓ Loaded: 8_marketing_analytics_platform_demo.md (1833 chars)\n",
      "✓ Loaded: 9_1on1_career_development_emma_davis.md (2196 chars)\n",
      "\n",
      "Successfully loaded 9 documents\n"
     ]
    }
   ],
   "source": [
    "DOC_PATH = \"data/meeting_summaries\"\n",
    "\n",
    "# Load the documents\n",
    "file_contents, file_paths = load_markdown_files(DOC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minions.minions import Minions\n",
    "from minions.clients.ollama import OllamaClient\n",
    "from minions.clients.openai import OpenAIClient\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class StructuredLocalOutput(BaseModel):\n",
    "    explanation: str\n",
    "    citation: str | None\n",
    "    answer: str | None\n",
    "\n",
    "LOCAL_MODEL_NAME = \"qwen2.5:3b\"\n",
    "REMOTE_MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "local_client = OllamaClient(\n",
    "                        model_name=LOCAL_MODEL_NAME,\n",
    "                        temperature=0.0,\n",
    "                        max_tokens=500,\n",
    "                        num_ctx=4096,\n",
    "                        use_async=False, # TODO: consider changing to True\n",
    "                        structured_output_schema=StructuredLocalOutput\n",
    "                    )\n",
    "                    \n",
    "\n",
    "remote_client = OpenAIClient(\n",
    "    model_name=REMOTE_MODEL_NAME,\n",
    "    temperature=0.0,\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "protocol = Minions(local_client, remote_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concat all the files into a single string using a \"---\" separator and the file paths as titles for each section \n",
    "# filenames_and_contents = []\n",
    "# for filename, content in zip(file_paths, file_contents):\n",
    "#     filenames_and_contents.append(f\"Filename: {filename}\")\n",
    "#     filenames_and_contents.append(content)\n",
    "#     filenames_and_contents.append(\"---DOC_SEPARATOR---\")\n",
    "\n",
    "# # join the list into a single string\n",
    "# filenames_and_contents = \"\\n\".join(filenames_and_contents)\n",
    "\n",
    "# # print the result\n",
    "# print(filenames_and_contents[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1. Q3 Marketing Strategy Review - Meeting Summary\n",
      "\n",
      "**Date:** October 15, 2024  \n",
      "**Time:** 2:00 PM - 3:00 PM EST  \n",
      "**Attendees:** Sarah Chen (VP Marketing), Mike Rodriguez (Digital Marketing Manager), Lisa Wang (Content Lead), James Mitchell (Analytics Manager), Emma Davis (Social Media Manager)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from minions.minions import Document\n",
    "document_list = [Document(content=content, filename=filename) for filename, content in zip(file_paths, file_contents)]\n",
    "print(document_list[0].content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zoom AI companion meeting summaries with the following filename metadata: data/meeting_summaries/1_q3_marketing_strategy_review.md, data/meeting_summaries/2_1on1_performance_review_mike_rodriguez.md, data/meeting_summaries/3_marketing_sales_alignment_meeting.md, data/meeting_summaries/4_website_redesign_vendor_evaluation.md, data/meeting_summaries/5_1on1_checkin_lisa_wang.md, data/meeting_summaries/6_executive_leadership_team_monthly_update.md, data/meeting_summaries/7_crisis_communication_planning_session.md, data/meeting_summaries/8_marketing_analytics_platform_demo.md, data/meeting_summaries/9_1on1_career_development_emma_davis.md\n",
      "\n",
      "========== MINIONS TASK STARTED ==========\n",
      "Task: what is Lisa Wang's role in the company?\n",
      "Max rounds: 5\n",
      "Retrieval: None\n",
      "Round 1/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[Document],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    job_manifests = []\n",
      "    \n",
      "    # Chunk the document by section to ensure atomic tasks\n",
      "    for document in context:\n",
      "        sections = chunk_by_section(document.content)\n",
      "        for section in sections:\n",
      "            job_manifest = JobManifest(\n",
      "                chunk=section,\n",
      "                task=\"Identify Lisa Wang's role in the company based on the provided text.\",\n",
      "                advice=\"Look for mentions of her job title, responsibilities, or contributions.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest)\n",
      "    \n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[Job],\n",
      ") -> str:\n",
      "    results = []\n",
      "    \n",
      "    for job in jobs:\n",
      "        if job.output.answer:  # Filter out any jobs without an answer\n",
      "            results.append(job.output.answer)\n",
      "    \n",
      "    # Aggregate the results into a single string\n",
      "    return \" \".join(results)\n",
      "```\n",
      "Created 9 job manifests (9 chunks, apriori requested 1 samples per chunk, 1 tasks)\n",
      "Total number of job_manifests: 9\n",
      "Sending 9 worker chats to the worker client\n",
      "\n",
      "========== WORKER OUTPUTS (Round 1) ==========\n",
      "\n",
      "--- Worker 0/1 (Job ID: 0) ---\n",
      "Task ID: 0 | Chunk ID: 0\n",
      "Task: Identify Lisa Wang's role in the company based on the provided text.\n",
      "Chunk Preview: ## 1. Q3 Marketing Strategy Review - Meeting Summary\n",
      "\n",
      "**Date:** October 15, 2024  \n",
      "**Time:** 2:00 PM...\n",
      "Answer: None\n",
      "Explanation: The text does not mention Lisa Wang's role in the company.\n",
      "Citation: None of the provided documents contain any information about a person named Lisa Wang or her role within the company.\n",
      "--------------------------------------------------\n",
      "========== END WORKER OUTPUTS (Round 1) ==========\n",
      "\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"No information has been gathered about Lisa Wang's role in the company, as there are no collected job outputs.\",\n",
      "  \"feedback\": \"We need to gather information regarding Lisa Wang's job title, responsibilities, or contributions.\",\n",
      "  \"decision\": \"request_additional_info\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Currently, we have no insights into Lisa Wang's role. Additional tasks need to be assigned to gather relevant information.\"\n",
      "}\n",
      "Round 2/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "Here are the implementations for the `prepare_jobs` and `transform_outputs` functions based on the provided specifications:\n",
      "\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[Document],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    task_id = 1  # Unique identifier for the task\n",
      "    job_manifests = []\n",
      "\n",
      "    # If there are previous job outputs, use them to create new job manifests\n",
      "    if prev_job_outputs:\n",
      "        for output in prev_job_outputs:\n",
      "            # Create a task for extracting Lisa Wang's role\n",
      "            task = \"Extract Lisa Wang's role in the company from the provided summaries.\"\n",
      "            job_manifest = JobManifest(\n",
      "                chunk=output.answer,\n",
      "                task=task,\n",
      "                advice=\"Look for mentions of job title, responsibilities, or contributions.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest)\n",
      "    \n",
      "    # If context is provided, chunk it and create job manifests\n",
      "    for document in context:\n",
      "        chunks = chunk_by_section(document.content)\n",
      "        for chunk in chunks:\n",
      "            task = \"Extract Lisa Wang's role in the company from the provided summaries.\"\n",
      "            job_manifest = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task,\n",
      "                advice=\"Focus on identifying job title and responsibilities.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest)\n",
      "\n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[Job],\n",
      ") -> str:\n",
      "    def filter_fn(job):\n",
      "        answer = job.output.answer\n",
      "        return answer is not None and str(answer).lower().strip() != \"none\" and answer != \"null\"\n",
      "    \n",
      "    # Filter jobs\n",
      "    for job in jobs:\n",
      "        job.include = filter_fn(job)\n",
      "    \n",
      "    # Aggregate and filter jobs\n",
      "    tasks = {}\n",
      "    for job in jobs:\n",
      "        task_id = job.manifest.task_id\n",
      "        chunk_id = job.manifest.chunk_id\n",
      "        \n",
      "        if task_id not in tasks:\n",
      "            tasks[task_id] = {\n",
      "                \"task_id\": task_id,\n",
      "                \"task\": job.manifest.task,\n",
      "                \"chunks\": {},\n",
      "            }\n",
      "        \n",
      "        if chunk_id not in tasks[task_id][\"chunks\"]:\n",
      "            tasks[task_id][\"chunks\"][chunk_id] = []\n",
      "        \n",
      "        tasks[task_id][\"chunks\"][chunk_id].append(job)\n",
      "    \n",
      "    # Build the aggregated string\n",
      "    aggregated_str = \"\"\n",
      "    for task_id, task_info in tasks.items():\n",
      "        aggregated_str += f\"## Task (task_id=`{task_id}`): {task_info['task']}\\n\\n\"\n",
      "        \n",
      "        for chunk_id, chunk_jobs in task_info[\"chunks\"].items():\n",
      "            filtered_jobs = [j for j in chunk_jobs if j.include]\n",
      "            \n",
      "            aggregated_str += f\"### Chunk # {chunk_id}\\n\"\n",
      "            if filtered_jobs:\n",
      "                for idx, job in enumerate(filtered_jobs, start=1):\n",
      "                    aggregated_str += f\"   -- Job {idx} (job_id=`{job.manifest.job_id}`):\\n\"\n",
      "                    aggregated_str += f\"   {job.output.answer}\\n\\n\"\n",
      "            else:\n",
      "                aggregated_str += \"   No jobs returned successfully for this chunk.\\n\\n\"\n",
      "        \n",
      "        aggregated_str += \"\\n-----------------------\\n\\n\"\n",
      "    \n",
      "    return aggregated_str\n",
      "```\n",
      "\n",
      "In the `prepare_jobs` function, we create job manifests based on previous job outputs and the context provided. Each manifest is tasked with extracting Lisa Wang's role from the summaries. In the `transform_outputs` function, we filter the jobs based on their outputs and aggregate the results into a structured string format for analysis.\n",
      "Created 10 job manifests (10 chunks, apriori requested 1 samples per chunk, 1 tasks)\n",
      "Total number of job_manifests: 10\n",
      "Sending 10 worker chats to the worker client\n",
      "\n",
      "========== WORKER OUTPUTS (Round 2) ==========\n",
      "\n",
      "--- Worker 0/1 (Job ID: 0) ---\n",
      "Task ID: 0 | Chunk ID: 0\n",
      "Task: Extract Lisa Wang's role in the company from the provided summaries.\n",
      "Chunk Preview: None\n",
      "Answer: None\n",
      "Explanation: The provided document excerpts do not contain any mention of Lisa Wang's role in the company.\n",
      "Citation: None\n",
      "--------------------------------------------------\n",
      "========== END WORKER OUTPUTS (Round 2) ==========\n",
      "\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"We have not gathered any relevant data regarding Lisa Wang's role, as no jobs returned successfully for the extraction task.\",\n",
      "  \"feedback\": \"We need to gather specific information about Lisa Wang's job title, responsibilities, or contributions within the company.\",\n",
      "  \"decision\": \"request_additional_info\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Current analysis shows no insights into Lisa Wang's role. Additional tasks are required to extract relevant information from meeting summaries.\"\n",
      "}\n",
      "Round 3/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "Here are the implementations for the `prepare_jobs` and `transform_outputs` functions based on the provided specifications:\n",
      "\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[Document],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    task_id = 1  # Unique identifier for the task\n",
      "    job_manifests = []\n",
      "\n",
      "    # If there are previous job outputs, we will use them to create new job manifests\n",
      "    if prev_job_outputs:\n",
      "        for output in prev_job_outputs:\n",
      "            # Create a task for extracting Lisa Wang's role\n",
      "            task = \"Extract Lisa Wang's role in the company from the provided summaries.\"\n",
      "            job_manifest = JobManifest(\n",
      "                chunk=output.answer,  # Using the answer from previous outputs as the chunk\n",
      "                task=task,\n",
      "                advice=\"Look for mentions of Lisa Wang's job title, responsibilities, or contributions.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest)\n",
      "\n",
      "    # If context is provided, we can chunk it and create additional job manifests\n",
      "    for document in context:\n",
      "        chunks = chunk_by_section(document.content)\n",
      "        for chunk in chunks:\n",
      "            task = \"Extract Lisa Wang's role in the company from the provided document chunk.\"\n",
      "            job_manifest = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task,\n",
      "                advice=\"Focus on identifying Lisa Wang's job title and responsibilities.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest)\n",
      "\n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[Job],\n",
      ") -> str:\n",
      "    def filter_fn(job):\n",
      "        answer = job.output.answer\n",
      "        return answer is not None and str(answer).lower().strip() != \"none\" and answer != \"null\"\n",
      "    \n",
      "    # Filter jobs\n",
      "    for job in jobs:\n",
      "        job.include = filter_fn(job)\n",
      "    \n",
      "    # Aggregate and filter jobs\n",
      "    tasks = {}\n",
      "    for job in jobs:\n",
      "        task_id = job.manifest.task_id\n",
      "        chunk_id = job.manifest.chunk_id\n",
      "        \n",
      "        if task_id not in tasks:\n",
      "            tasks[task_id] = {\n",
      "                \"task_id\": task_id,\n",
      "                \"task\": job.manifest.task,\n",
      "                \"chunks\": {},\n",
      "            }\n",
      "        \n",
      "        if chunk_id not in tasks[task_id][\"chunks\"]:\n",
      "            tasks[task_id][\"chunks\"][chunk_id] = []\n",
      "        \n",
      "        tasks[task_id][\"chunks\"][chunk_id].append(job)\n",
      "    \n",
      "    # Build the aggregated string\n",
      "    aggregated_str = \"\"\n",
      "    for task_id, task_info in tasks.items():\n",
      "        aggregated_str += f\"## Task (task_id=`{task_id}`): {task_info['task']}\\n\\n\"\n",
      "        \n",
      "        for chunk_id, chunk_jobs in task_info[\"chunks\"].items():\n",
      "            filtered_jobs = [j for j in chunk_jobs if j.include]\n",
      "            \n",
      "            aggregated_str += f\"### Chunk # {chunk_id}\\n\"\n",
      "            if filtered_jobs:\n",
      "                for idx, job in enumerate(filtered_jobs, start=1):\n",
      "                    aggregated_str += f\"   -- Job {idx} (job_id=`{job.manifest.job_id}`):\\n\"\n",
      "                    aggregated_str += f\"   {job.output.answer}\\n\\n\"\n",
      "            else:\n",
      "                aggregated_str += \"   No jobs returned successfully for this chunk.\\n\\n\"\n",
      "        \n",
      "        aggregated_str += \"\\n-----------------------\\n\\n\"\n",
      "    \n",
      "    return aggregated_str\n",
      "```\n",
      "\n",
      "In the `prepare_jobs` function, we create job manifests based on previous job outputs and the context provided. The `transform_outputs` function filters the jobs based on their outputs and aggregates the results into a structured string format.\n",
      "Created 10 job manifests (10 chunks, apriori requested 1 samples per chunk, 2 tasks)\n",
      "Total number of job_manifests: 10\n",
      "Sending 10 worker chats to the worker client\n",
      "\n",
      "========== WORKER OUTPUTS (Round 3) ==========\n",
      "\n",
      "--- Worker 0/1 (Job ID: 0) ---\n",
      "Task ID: 0 | Chunk ID: 0\n",
      "Task: Extract Lisa Wang's role in the company from the provided summaries.\n",
      "Chunk Preview: None\n",
      "Answer: None\n",
      "Explanation: The document excerpt focuses on a career development session for Emma Davis and does not mention Lisa Wang.\n",
      "Citation: None\n",
      "--------------------------------------------------\n",
      "========== END WORKER OUTPUTS (Round 3) ==========\n",
      "\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"No relevant information about Lisa Wang's role has been gathered from the job outputs, indicating that we cannot answer the question at this time.\",\n",
      "  \"feedback\": \"Look for mentions of Lisa Wang's job title, responsibilities, or contributions in the meeting summaries.\",\n",
      "  \"decision\": \"request_additional_info\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Current analysis shows no insights into Lisa Wang's role. Additional tasks are required to extract relevant information from meeting summaries.\"\n",
      "}\n",
      "Round 4/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "Here are the implementations for the `prepare_jobs` and `transform_outputs` functions based on the provided specifications:\n",
      "\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[Document],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    task_id = 1  # Unique identifier for the task\n",
      "    job_manifests = []\n",
      "\n",
      "    # If there are previous job outputs, use them to create new job manifests\n",
      "    if prev_job_outputs:\n",
      "        for output in prev_job_outputs:\n",
      "            # Create a task for extracting Lisa Wang's role\n",
      "            task = \"Extract Lisa Wang's role in the company from the provided summaries.\"\n",
      "            job_manifest = JobManifest(\n",
      "                chunk=output.answer,  # Use the answer from previous outputs as the chunk\n",
      "                task=task,\n",
      "                advice=\"Look for mentions of Lisa Wang's job title or responsibilities.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest)\n",
      "    \n",
      "    # If context is provided, chunk it and create job manifests\n",
      "    for document in context:\n",
      "        chunks = chunk_by_section(document.content)\n",
      "        for chunk in chunks:\n",
      "            task = \"Extract Lisa Wang's role in the company from the document chunk.\"\n",
      "            job_manifest = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task,\n",
      "                advice=\"Focus on identifying Lisa Wang's job title or responsibilities.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest)\n",
      "\n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[Job],\n",
      ") -> str:\n",
      "    def filter_fn(job):\n",
      "        answer = job.output.answer\n",
      "        return answer is not None and str(answer).lower().strip() != \"none\" and answer != \"null\"\n",
      "    \n",
      "    # Filter jobs\n",
      "    for job in jobs:\n",
      "        job.include = filter_fn(job)\n",
      "    \n",
      "    # Aggregate and filter jobs\n",
      "    tasks = {}\n",
      "    for job in jobs:\n",
      "        task_id = job.manifest.task_id\n",
      "        chunk_id = job.manifest.chunk_id\n",
      "        \n",
      "        if task_id not in tasks:\n",
      "            tasks[task_id] = {\n",
      "                \"task_id\": task_id,\n",
      "                \"task\": job.manifest.task,\n",
      "                \"chunks\": {},\n",
      "            }\n",
      "        \n",
      "        if chunk_id not in tasks[task_id][\"chunks\"]:\n",
      "            tasks[task_id][\"chunks\"][chunk_id] = []\n",
      "        \n",
      "        tasks[task_id][\"chunks\"][chunk_id].append(job)\n",
      "    \n",
      "    # Build the aggregated string\n",
      "    aggregated_str = \"\"\n",
      "    for task_id, task_info in tasks.items():\n",
      "        aggregated_str += f\"## Task (task_id=`{task_id}`): {task_info['task']}\\n\\n\"\n",
      "        \n",
      "        for chunk_id, chunk_jobs in task_info[\"chunks\"].items():\n",
      "            filtered_jobs = [j for j in chunk_jobs if j.include]\n",
      "            \n",
      "            aggregated_str += f\"### Chunk # {chunk_id}\\n\"\n",
      "            if filtered_jobs:\n",
      "                for idx, job in enumerate(filtered_jobs, start=1):\n",
      "                    aggregated_str += f\"   -- Job {idx} (job_id=`{job.manifest.job_id}`):\\n\"\n",
      "                    aggregated_str += f\"   {job.output.answer}\\n\\n\"\n",
      "            else:\n",
      "                aggregated_str += \"   No jobs returned successfully for this chunk.\\n\\n\"\n",
      "        \n",
      "        aggregated_str += \"\\n-----------------------\\n\\n\"\n",
      "    \n",
      "    return aggregated_str\n",
      "```\n",
      "\n",
      "In the `prepare_jobs` function, we create job manifests based on previous job outputs and the current context. The `transform_outputs` function filters and aggregates the job outputs, formatting them into a structured string for analysis.\n",
      "Created 10 job manifests (10 chunks, apriori requested 1 samples per chunk, 2 tasks)\n",
      "Total number of job_manifests: 10\n",
      "Sending 10 worker chats to the worker client\n",
      "\n",
      "========== WORKER OUTPUTS (Round 4) ==========\n",
      "\n",
      "--- Worker 0/1 (Job ID: 0) ---\n",
      "Task ID: 0 | Chunk ID: 0\n",
      "Task: Extract Lisa Wang's role in the company from the provided summaries.\n",
      "Chunk Preview: None\n",
      "Answer: None\n",
      "Explanation: The document excerpt focuses on a career development session for Emma Davis and does not mention Lisa Wang.\n",
      "Citation: None\n",
      "--------------------------------------------------\n",
      "========== END WORKER OUTPUTS (Round 4) ==========\n",
      "\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"No relevant information about Lisa Wang's role was extracted from the meeting summaries, making it impossible to answer the question.\",\n",
      "  \"feedback\": \"Look for mentions of Lisa Wang's job title, responsibilities, or contributions in the meeting summaries.\",\n",
      "  \"decision\": \"request_additional_info\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Current analysis shows no insights into Lisa Wang's role. Additional tasks are required to extract relevant information from meeting summaries.\"\n",
      "}\n",
      "Round 5/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mzoom AI companion meeting summaries with the following filename metadata: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([doc.filename \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m document_list]))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m output = \u001b[43mprotocol\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhat is Lisa Wang\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms role in the company?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# \"what is the close rate for enterprise leads versus SMB leads?\",# \"how many languages are supported by the new website?\",\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdoc_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma list of short zoom AI companion meeting summaries\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# \"a list of short zoom AI companion meeting summaries with the following filenames: \" + \", \".join([doc.filename for doc in document_list]),\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocument_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# you can adjust rounds as needed for testing\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/Stanford/Dan/minions/minions/minions.py:538\u001b[39m, in \u001b[36mMinions.__call__\u001b[39m\u001b[34m(self, task, doc_metadata, context, max_rounds, max_jobs_per_round, num_tasks_per_round, num_samples_per_task, mcp_tools_info, use_retrieval, log_path, logging_id, retrieval_model, chunk_fn, chunk_params)\u001b[39m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28mself\u001b[39m.callback(\u001b[33m\"\u001b[39m\u001b[33msupervisor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, is_final=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    537\u001b[39m remote_start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m task_response, usage = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mremote_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43msupervisor_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m current_time = time.time()\n\u001b[32m    542\u001b[39m timing[\u001b[33m\"\u001b[39m\u001b[33mremote_call_time\u001b[39m\u001b[33m\"\u001b[39m] += current_time - remote_start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/Stanford/Dan/minions/minions/clients/openai.py:160\u001b[39m, in \u001b[36mOpenAIClient.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mo1\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mo3\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_name:\n\u001b[32m    158\u001b[39m         params[\u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.reasoning_effort\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError during OpenAI API call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/openai/_base_client.py:969\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    967\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    975\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"zoom AI companion meeting summaries with the following filename metadata: \" + \", \".join([doc.filename for doc in document_list]))\n",
    "\n",
    "output = protocol(\n",
    "        task= \"what is Lisa Wang's role in the company?\",# \"what is the close rate for enterprise leads versus SMB leads?\",# \"how many languages are supported by the new website?\",\n",
    "        doc_metadata= \"a list of short zoom AI companion meeting summaries\",# \"a list of short zoom AI companion meeting summaries with the following filenames: \" + \", \".join([doc.filename for doc in document_list]),\n",
    "        context=document_list,\n",
    "        max_rounds=5,  # you can adjust rounds as needed for testing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== MINIONS TASK STARTED ==========\n",
      "Task: how many 1:1 meetings did I hold in total?\n",
      "Max rounds: 5\n",
      "Retrieval: None\n",
      "Round 1/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[Document],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    job_manifests = []\n",
      "    \n",
      "    for document in context:\n",
      "        chunks = chunk_by_section(document.content, max_chunk_size=500, overlap=20)\n",
      "        \n",
      "        for chunk in chunks:\n",
      "            if \"1on1\" in document.filename:\n",
      "                task = \"Extract the names of individuals involved in the 1:1 meeting.\"\n",
      "                advice = \"Look for names mentioned in the context of the meeting.\"\n",
      "                job_manifests.append(JobManifest(chunk=chunk, task=task, advice=advice))\n",
      "    \n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[JobOutput],\n",
      ") -> str:\n",
      "    aggregated_results = {}\n",
      "    \n",
      "    for job in jobs:\n",
      "        if job.answer:\n",
      "            task_id = job.manifest.task_id\n",
      "            if task_id not in aggregated_results:\n",
      "                aggregated_results[task_id] = []\n",
      "            aggregated_results[task_id].append(job.answer)\n",
      "    \n",
      "    final_output = []\n",
      "    for task_id, answers in aggregated_results.items():\n",
      "        combined_answers = \", \".join(answers)\n",
      "        final_output.append(f\"Task ID {task_id}: {combined_answers}\")\n",
      "    \n",
      "    return \"\\n\".join(final_output)\n",
      "```\n",
      "Created 13 job manifests (13 chunks, apriori requested 1 samples per chunk, 1 tasks)\n",
      "Total number of job_manifests: 13\n",
      "Sending 13 worker chats to the worker client\n",
      "None\n",
      "Error executing transformation code: AttributeError: 'Job' object has no attribute 'answer'\n",
      "After filtering, 0/1 jobs were included\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"No information was successfully extracted regarding the individuals involved in the 1:1 meetings, making it impossible to determine the total number of such meetings held.\",\n",
      "  \"feedback\": null,\n",
      "  \"decision\": \"request_additional_info\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Collected outputs indicate no successful extractions for the task of identifying individuals in 1:1 meetings. We need to reassign the task to gather necessary details.\"\n",
      "}\n",
      "Round 2/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "Here's the implementation of the two functions, `prepare_jobs` and `transform_outputs`, tailored to the requirements of the task at hand.\n",
      "\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[Document],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    task_id = 1  # Unique identifier for the task\n",
      "    job_manifests = []\n",
      "\n",
      "    # Extract the content from the context documents\n",
      "    for document in context:\n",
      "        # Chunk the document content for processing\n",
      "        chunks = chunk_by_section(document.content)\n",
      "        \n",
      "        # Create tasks for each chunk to identify 1:1 meetings\n",
      "        for chunk in chunks:\n",
      "            task = \"Identify if this chunk contains a mention of a 1:1 meeting.\"\n",
      "            job_manifest = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task,\n",
      "                advice=\"Look for keywords like '1on1' or 'one-on-one' in the text.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest)\n",
      "\n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[Job],\n",
      ") -> str:\n",
      "    def filter_fn(job):\n",
      "        # Filter for jobs that mention 1:1 meetings\n",
      "        return \"1on1\" in job.output.answer.lower() if job.output.answer else False\n",
      "    \n",
      "    # Filter jobs\n",
      "    for job in jobs:\n",
      "        job.include = filter_fn(job)\n",
      "    \n",
      "    # Aggregate and filter jobs\n",
      "    tasks = {}\n",
      "    for job in jobs:\n",
      "        task_id = job.manifest.task_id\n",
      "        chunk_id = job.manifest.chunk_id\n",
      "        \n",
      "        if task_id not in tasks:\n",
      "            tasks[task_id] = {\n",
      "                \"task_id\": task_id,\n",
      "                \"task\": job.manifest.task,\n",
      "                \"chunks\": {},\n",
      "            }\n",
      "        \n",
      "        if chunk_id not in tasks[task_id][\"chunks\"]:\n",
      "            tasks[task_id][\"chunks\"][chunk_id] = []\n",
      "        \n",
      "        tasks[task_id][\"chunks\"][chunk_id].append(job)\n",
      "    \n",
      "    # Build the aggregated string\n",
      "    aggregated_str = \"\"\n",
      "    for task_id, task_info in tasks.items():\n",
      "        aggregated_str += f\"## Task (task_id=`{task_id}`): {task_info['task']}\\n\\n\"\n",
      "        \n",
      "        for chunk_id, chunk_jobs in task_info[\"chunks\"].items():\n",
      "            filtered_jobs = [j for j in chunk_jobs if j.include]\n",
      "            \n",
      "            aggregated_str += f\"### Chunk # {chunk_id}\\n\"\n",
      "            if filtered_jobs:\n",
      "                for idx, job in enumerate(filtered_jobs, start=1):\n",
      "                    aggregated_str += f\"   -- Job {idx} (job_id=`{job.manifest.job_id}`):\\n\"\n",
      "                    aggregated_str += f\"   {job.sample}\\n\"\n",
      "            else:\n",
      "                aggregated_str += \"   No jobs returned successfully for this chunk.\\n\"\n",
      "        \n",
      "        aggregated_str += \"\\n-----------------------\\n\"\n",
      "    \n",
      "    return aggregated_str\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **prepare_jobs**: This function prepares tasks for small language models by chunking the content of each document and creating a task to identify mentions of 1:1 meetings. Each chunk is processed independently to ensure atomicity.\n",
      "  \n",
      "- **transform_outputs**: This function filters the outputs from the small language models to include only those that mention 1:1 meetings. It aggregates the results by task and chunk, providing a structured summary of the findings. \n",
      "\n",
      "The implementation adheres to the constraints and requirements specified, ensuring that each task is atomic and that the outputs are aggregated effectively.\n",
      "Created 9 job manifests (9 chunks, apriori requested 1 samples per chunk, 1 tasks)\n",
      "Total number of job_manifests: 9\n",
      "Sending 9 worker chats to the worker client\n",
      "Yes\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"No successful extractions were made regarding 1:1 meetings, leaving us without sufficient information to answer the question.\",\n",
      "  \"feedback\": \"Reassign the task to specifically look for keywords indicating 1:1 meetings.\",\n",
      "  \"decision\": \"request_additional_info\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Collected outputs indicate no successful extractions for identifying individuals in 1:1 meetings. We need to gather necessary details to answer how many 1:1 meetings were held.\"\n",
      "}\n",
      "Round 3/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[Document],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    task_id = 1  # Unique identifier for the task\n",
      "    job_manifests = []\n",
      "\n",
      "    # Extract the content from the documents and chunk them\n",
      "    for document in context:\n",
      "        chunks = chunk_by_section(document.content)\n",
      "        for chunk in chunks:\n",
      "            # Create a task for identifying 1:1 meetings\n",
      "            task = \"Identify if this chunk contains a mention of a 1:1 meeting.\"\n",
      "            job_manifest = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task,\n",
      "                advice=\"Look for keywords like '1on1' or 'one-on-one' in the text.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest)\n",
      "    \n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[Job],\n",
      ") -> str:\n",
      "    def filter_fn(job):\n",
      "        # Filter for outputs that mention 1:1 meetings\n",
      "        answer = job.output.answer\n",
      "        return answer is not None and \"1on1\" in answer.lower()\n",
      "\n",
      "    # Filter jobs\n",
      "    for job in jobs:\n",
      "        job.include = filter_fn(job)\n",
      "    \n",
      "    # Aggregate and filter jobs\n",
      "    tasks = {}\n",
      "    for job in jobs:\n",
      "        task_id = job.manifest.task_id\n",
      "        chunk_id = job.manifest.chunk_id\n",
      "        \n",
      "        if task_id not in tasks:\n",
      "            tasks[task_id] = {\n",
      "                \"task_id\": task_id,\n",
      "                \"task\": job.manifest.task,\n",
      "                \"chunks\": {},\n",
      "            }\n",
      "        \n",
      "        if chunk_id not in tasks[task_id][\"chunks\"]:\n",
      "            tasks[task_id][\"chunks\"][chunk_id] = []\n",
      "        \n",
      "        tasks[task_id][\"chunks\"][chunk_id].append(job)\n",
      "    \n",
      "    # Build the aggregated string\n",
      "    aggregated_str = \"\"\n",
      "    for task_id, task_info in tasks.items():\n",
      "        aggregated_str += f\"## Task (task_id=`{task_id}`): {task_info['task']}\\n\\n\"\n",
      "        \n",
      "        for chunk_id, chunk_jobs in task_info[\"chunks\"].items():\n",
      "            filtered_jobs = [j for j in chunk_jobs if j.include]\n",
      "            \n",
      "            aggregated_str += f\"### Chunk # {chunk_id}\\n\"\n",
      "            if filtered_jobs:\n",
      "                for idx, job in enumerate(filtered_jobs, start=1):\n",
      "                    aggregated_str += f\"   -- Job {idx} (job_id=`{job.manifest.job_id}`):\\n\"\n",
      "                    aggregated_str += f\"   {job.sample}\\n\"\n",
      "            else:\n",
      "                aggregated_str += \"   No jobs returned successfully for this chunk.\\n\"\n",
      "        \n",
      "        aggregated_str += \"\\n-----------------------\\n\"\n",
      "    \n",
      "    return aggregated_str\n",
      "```\n",
      "Created 9 job manifests (9 chunks, apriori requested 1 samples per chunk, 1 tasks)\n",
      "Total number of job_manifests: 9\n",
      "Sending 9 worker chats to the worker client\n",
      "Yes\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"No successful extractions were made regarding 1:1 meetings, leaving us without any information to answer the question.\",\n",
      "  \"feedback\": \"Look for mentions of individuals involved in 1:1 meetings or explicit statements indicating their occurrence.\",\n",
      "  \"decision\": \"request_additional_info\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Collected outputs indicate no successful extractions for identifying individuals in 1:1 meetings. We need to gather necessary details to answer how many 1:1 meetings were held.\"\n",
      "}\n",
      "Round 4/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "Here's the implementation of the two functions `prepare_jobs` and `transform_outputs` based on the requirements provided:\n",
      "\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[Document],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    task_id = 1  # Unique identifier for the task\n",
      "    job_manifests = []\n",
      "\n",
      "    # Extract chunks from the context documents\n",
      "    for document in context:\n",
      "        chunks = chunk_by_section(document.content)\n",
      "        for chunk in chunks:\n",
      "            # Create a task for identifying 1:1 meetings\n",
      "            task = \"Identify if this chunk contains a mention of a 1:1 meeting.\"\n",
      "            job_manifest = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task,\n",
      "                advice=\"Look for keywords like '1on1' or 'one-on-one' in the text.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest)\n",
      "\n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[Job],\n",
      ") -> str:\n",
      "    def filter_fn(job):\n",
      "        # Filter for outputs that indicate a mention of a 1:1 meeting\n",
      "        return \"1on1\" in job.output.answer.lower() if job.output.answer else False\n",
      "    \n",
      "    # Filter jobs\n",
      "    for job in jobs:\n",
      "        job.include = filter_fn(job)\n",
      "    \n",
      "    # Aggregate and filter jobs\n",
      "    tasks = {}\n",
      "    for job in jobs:\n",
      "        task_id = job.manifest.task_id\n",
      "        chunk_id = job.manifest.chunk_id\n",
      "        \n",
      "        if task_id not in tasks:\n",
      "            tasks[task_id] = {\n",
      "                \"task_id\": task_id,\n",
      "                \"task\": job.manifest.task,\n",
      "                \"chunks\": {},\n",
      "            }\n",
      "        \n",
      "        if chunk_id not in tasks[task_id][\"chunks\"]:\n",
      "            tasks[task_id][\"chunks\"][chunk_id] = []\n",
      "        \n",
      "        tasks[task_id][\"chunks\"][chunk_id].append(job)\n",
      "    \n",
      "    # Build the aggregated string\n",
      "    aggregated_str = \"\"\n",
      "    for task_id, task_info in tasks.items():\n",
      "        aggregated_str += f\"## Task (task_id=`{task_id}`): {task_info['task']}\\n\\n\"\n",
      "        \n",
      "        for chunk_id, chunk_jobs in task_info[\"chunks\"].items():\n",
      "            filtered_jobs = [j for j in chunk_jobs if j.include]\n",
      "            \n",
      "            aggregated_str += f\"### Chunk # {chunk_id}\\n\"\n",
      "            if filtered_jobs:\n",
      "                for idx, job in enumerate(filtered_jobs, start=1):\n",
      "                    aggregated_str += f\"   -- Job {idx} (job_id=`{job.manifest.job_id}`):\\n\"\n",
      "                    aggregated_str += f\"   {job.sample}\\n\"\n",
      "            else:\n",
      "                aggregated_str += \"   No jobs returned successfully for this chunk.\\n\"\n",
      "        \n",
      "        aggregated_str += \"\\n-----------------------\\n\"\n",
      "    \n",
      "    return aggregated_str\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **prepare_jobs**: This function prepares jobs by chunking the content of each document and creating a task to identify mentions of 1:1 meetings. Each chunk is processed individually, ensuring that the tasks are atomic.\n",
      "  \n",
      "- **transform_outputs**: This function filters the outputs based on whether they mention a 1:1 meeting and aggregates the results. It constructs a structured string that summarizes the findings from the small language models, making it easy to review the results.\n",
      "Created 9 job manifests (9 chunks, apriori requested 1 samples per chunk, 1 tasks)\n",
      "Total number of job_manifests: 9\n",
      "Sending 9 worker chats to the worker client\n",
      "Yes\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"No successful extractions were made regarding 1:1 meetings from the collected outputs, leaving us without the necessary information to answer the question.\",\n",
      "  \"feedback\": null,\n",
      "  \"decision\": \"request_additional_info\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Collected outputs indicate no successful extractions for identifying individuals in 1:1 meetings. We need to gather necessary details to answer how many 1:1 meetings were held.\"\n",
      "}\n",
      "Round 5/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mminions\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mminions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m output = \u001b[43mprotocol\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhow many 1:1 meetings did I hold in total?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# \"how many languages are supported by the new website?\",\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdoc_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma list of short zoom AI companion meeting summaries with the following file paths: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_contents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# you can adjust rounds as needed for testing\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/Stanford/Dan/minions/minions/minions.py:498\u001b[39m, in \u001b[36mMinions.__call__\u001b[39m\u001b[34m(self, task, doc_metadata, context, max_rounds, max_jobs_per_round, num_tasks_per_round, num_samples_per_task, mcp_tools_info, use_retrieval, log_path, logging_id, retrieval_model, chunk_fn, chunk_params)\u001b[39m\n\u001b[32m    495\u001b[39m     \u001b[38;5;28mself\u001b[39m.callback(\u001b[33m\"\u001b[39m\u001b[33msupervisor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, is_final=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    497\u001b[39m remote_start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m task_response, usage = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mremote_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43msupervisor_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m current_time = time.time()\n\u001b[32m    502\u001b[39m timing[\u001b[33m\"\u001b[39m\u001b[33mremote_call_time\u001b[39m\u001b[33m\"\u001b[39m] += current_time - remote_start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/Stanford/Dan/minions/minions/clients/openai.py:160\u001b[39m, in \u001b[36mOpenAIClient.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mo1\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mo3\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_name:\n\u001b[32m    158\u001b[39m         params[\u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.reasoning_effort\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError during OpenAI API call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/openai/_base_client.py:969\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    967\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    975\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/minions-py311/lib/python3.11/ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from minions.minions import Document\n",
    "output = protocol(\n",
    "        task= \"how many 1:1 meetings did I hold in total?\", # \"how many languages are supported by the new website?\",\n",
    "        doc_metadata=\"a list of short zoom AI companion meeting summaries with the following file paths: \" + \", \".join(file_paths),\n",
    "        context=[Document(content=content, filename=filename) for filename, content in zip(file_paths, file_contents)],\n",
    "        max_rounds=5,  # you can adjust rounds as needed for testing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minions-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
