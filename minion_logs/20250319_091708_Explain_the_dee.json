{
  "task": "Explain the deep learning architecture used?",
  "context": "\n--------------------\n### Content from s41586-021-03819-2.pdf:\nNature  |  Vol 596  |  26 August 2021  |  583\nArticle\nHighly accurate protein structure prediction \nwith AlphaFold\nJohn Jumper1,4 ✉, Richard Evans1,4, Alexander Pritzel1,4, Tim Green1,4, Michael Figurnov1,4, \nOlaf Ronneberger1,4, Kathryn Tunyasuvunakool1,4, Russ Bates1,4, Augustin Žídek1,4, \nAnna Potapenko1,4, Alex Bridgland1,4, Clemens Meyer1,4, Simon A. A. Kohl1,4, \nAndrew J. Ballard1,4, Andrew Cowie1,4, Bernardino Romera-Paredes1,4, Stanislav Nikolov1,4, \nRishub Jain1,4, Jonas Adler1, Trevor Back1, Stig Petersen1, David Reiman1, Ellen Clancy1, \nMichal Zielinski1, Martin Steinegger2,3, Michalina Pacholska1, Tamas Berghammer1, \nSebastian Bodenstein1, David Silver1, Oriol Vinyals1, Andrew W. Senior1, Koray Kavukcuoglu1, \nPushmeet Kohli1 & Demis Hassabis1,4 ✉\nProteins are essential to life, and understanding their structure can facilitate a \nmechanistic understanding of their function. Through an enormous experimental \neffort1–4, the structures of around 100,000 unique proteins have been determined5, but \nthis represents a small fraction of the billions of known protein sequences6,7. Structural \ncoverage is bottlenecked by the months to years of painstaking effort required to \ndetermine a single protein structure. Accurate computational approaches are needed \nto address this gap and to enable large-scale structural bioinformatics. Predicting the \nthree-dimensional structure that a protein will adopt based solely on its amino acid \nsequence—the structure prediction component of the ‘protein folding problem’8—has \nbeen an important open research problem for more than 50 years9. Despite recent \nprogress10–14, existing methods fall far short of atomic accuracy, especially when no \nhomologous structure is available. Here we provide the first computational method \nthat can regularly predict protein structures with atomic accuracy even in cases in which \nno similar structure is known. We validated an entirely redesigned version of our neural \nnetwork-based model, AlphaFold, in the challenging 14th Critical Assessment of protein \nStructure Prediction (CASP14)15, demonstrating accuracy competitive with \nexperimental structures in a majority of cases and greatly outperforming other \nmethods. Underpinning the latest version of AlphaFold is a novel machine learning \napproach that incorporates physical and biological knowledge about protein structure, \nleveraging multi-sequence alignments, into the design of the deep learning algorithm.\nThe development of computational methods to predict \nthree-dimensional (3D) protein structures from the protein sequence \nhas proceeded along two complementary paths that focus on either the \nphysical interactions or the evolutionary history. The physical interac-\ntion programme heavily integrates our understanding of molecular \ndriving forces into either thermodynamic or kinetic simulation of pro-\ntein physics16 or statistical approximations thereof17. Although theoreti-\ncally very appealing, this approach has proved highly challenging for \neven moderate-sized proteins due to the computational intractability \nof molecular simulation, the context dependence of protein stability \nand the difficulty of producing sufficiently accurate models of protein \nphysics. The evolutionary programme has provided an alternative in \nrecent years, in which the constraints on protein structure are derived \nfrom bioinformatics analysis of the evolutionary history of proteins, \nhomology to solved structures18,19 and pairwise evolutionary correla-\ntions20–24. This bioinformatics approach has benefited greatly from \nthe steady growth of experimental protein structures deposited in \nthe Protein Data Bank (PDB)5, the explosion of genomic sequencing \nand the rapid development of deep learning techniques to interpret \nthese correlations. Despite these advances, contemporary physical \nand evolutionary-history-based approaches produce predictions that \nare far short of experimental accuracy in the majority of cases in which \na close homologue has not been solved experimentally and this has \nlimited their utility for many biological applications.\nIn this study, we develop the first, to our knowledge, computational \napproach capable of predicting protein structures to near experimental \naccuracy in a majority of cases. The neural network AlphaFold that we \ndeveloped was entered into the CASP14 assessment (May–July 2020; \nentered under the team name ‘AlphaFold2’ and a completely different \nmodel from our CASP13 AlphaFold system10). The CASP assessment is \ncarried out biennially using recently solved structures that have not \nbeen deposited in the PDB or publicly disclosed so that it is a blind test \nhttps://doi.org/10.1038/s41586-021-03819-2\nReceived: 11 May 2021\nAccepted: 12 July 2021\nPublished online: 15 July 2021\nOpen access\n Check for updates\n1DeepMind, London, UK. 2School of Biological Sciences, Seoul National University, Seoul, South Korea. 3Artificial Intelligence Institute, Seoul National University, Seoul, South Korea. 4These \nauthors contributed equally: John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna \nPotapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Demis Hassabis.  \n✉e-mail: jumper@deepmind.com; dhcontact@deepmind.com\n584  |  Nature  |  Vol 596  |  26 August 2021\nArticle\nfor the participating methods, and has long served as the gold-standard \nassessment for the accuracy of structure prediction25,26.\nIn CASP14, AlphaFold structures were vastly more accurate than \ncompeting methods. AlphaFold structures had a median backbone \naccuracy of 0.96 Å r.m.s.d.95 (Cα root-mean-square deviation at 95% \nresidue coverage) (95% confidence interval = 0.85–1.16 Å) whereas \nthe next best performing method had a median backbone accuracy \nof 2.8 Å r.m.s.d.95 (95% confidence interval = 2.7–4.0 Å) (measured on \nCASP domains; see Fig. 1a for backbone accuracy and Supplementary \nFig. 14 for all-atom accuracy). As a comparison point for this accuracy, \nthe width of a carbon atom is approximately 1.4 Å. In addition to very \naccurate domain structures (Fig. 1b), AlphaFold is able to produce \nhighly accurate side chains (Fig. 1c) when the backbone is highly accu-\nrate and considerably improves over template-based methods even \nwhen strong templates are available. The all-atom accuracy of Alpha-\nFold was 1.5 Å r.m.s.d.95 (95% confidence interval = 1.2–1.6 Å) compared \nwith the 3.5 Å r.m.s.d.95 (95% confidence interval = 3.1–4.2 Å) of the best \nalternative method. Our methods are scalable to very long proteins with \naccurate domains and domain-packing (see Fig. 1d for the prediction \nof a 2,180-residue protein with no structural homologues). Finally, the \nmodel is able to provide precise, per-residue estimates of its reliability \nthat should enable the confident use of these predictions.\nWe demonstrate in Fig. 2a that the high accuracy that AlphaFold dem-\nonstrated in CASP14 extends to a large sample of recently released PDB \nstructures; in this dataset, all structures were deposited in the PDB after \nour training data cut-off and are analysed as full chains (see Methods, \nSupplementary Fig. 15 and Supplementary Table 6 for more details). \nFurthermore, we observe high side-chain accuracy when the back-\nbone prediction is accurate (Fig. 2b) and we show that our confidence \nmeasure, the predicted local-distance difference test (pLDDT), reliably \npredicts the Cα local-distance difference test (lDDT-Cα) accuracy of the \ncorresponding prediction (Fig. 2c). We also find that the global super-\nposition metric template modelling score (TM-score)27 can be accu-\nrately estimated (Fig. 2d). Overall, these analyses validate that the high \naccuracy and reliability of AlphaFold on CASP14 proteins also transfers \nto an uncurated collection of recent PDB submissions, as would be \nexpected (see Supplementary Methods 1.15 and Supplementary Fig. 11 \nfor confirmation that this high accuracy extends to new folds).\nThe AlphaFold network\nAlphaFold greatly improves the accuracy of structure prediction by \nincorporating novel neural network architectures and training proce-\ndures based on the evolutionary, physical and geometric constraints \nof protein structures. In particular, we demonstrate a new architecture \nto jointly embed multiple sequence alignments (MSAs) and pairwise \nfeatures, a new output representation and associated loss that enable \naccurate end-to-end structure prediction, a new equivariant attention \na\nG427\nAlphaFold\nG009\nG473\nG129\nG403\nG032\nG420\nG480\nG498\nG488\nG368\nG324\nG362\nG253\nG216\n0\n1\n2\n3\n4\nMedian Cα r.m.s.d.95 (Å)\nb\nC terminus\nN terminus\nAlphaFold\nExperiment\nr.m.s.d.95 = 0.8 Å; TM-score = 0.93\nc\nAlphaFold\nExperiment\nr.m.s.d. = 0.59 Å within 8 Å of Zn\nd\nAlphaFold\nExperiment\nr.m.s.d.95 = 2.2 Å; TM-score = 0.96\nMSA\n3D structure\nLow \nconﬁdence\nHigh \nconﬁdence\nTemplates\nInput sequence\nMSA \nrepresentation\n(s,r,c)\nEvoformer \n(48 blocks)\nStructure \nmodule\n (8 blocks)\n+\n+\n← Recycling (three times)\nPairing\nPair \nrepresentation\n(r,r,c)\nPair \nrepresentation\n(r,r,c)\nGenetic \ndatabase\nsearch\nStructure \ndatabase\nsearch\ne\nSingle repr. (r,c)\nFig. 1 | AlphaFold produces highly accurate structures. a, The performance \nof AlphaFold on the CASP14 dataset (n = 87 protein domains) relative to the top-\n15 entries (out of 146 entries), group numbers correspond to the numbers \nassigned to entrants by CASP. Data are median and the 95% confidence interval \nof the median, estimated from 10,000 bootstrap samples. b, Our prediction of \nCASP14 target T1049 (PDB 6Y4F, blue) compared with the true (experimental) \nstructure (green). Four residues in the C terminus of the crystal structure are \nB-factor outliers and are not depicted. c, CASP14 target T1056 (PDB 6YJ1).  \nAn example of a well-predicted zinc-binding site (AlphaFold has accurate side \nchains even though it does not explicitly predict the zinc ion). d, CASP target \nT1044 (PDB 6VR4)—a 2,180-residue single chain—was predicted with correct \ndomain packing (the prediction was made after CASP using AlphaFold without \nintervention). e, Model architecture. Arrows show the information flow among \nthe various components described in this paper. Array shapes are shown in \nparentheses with s, number of sequences (Nseq in the main text); r, number of \nresidues (Nres in the main text); c, number of channels.\nNature  |  Vol 596  |  26 August 2021  |  585\narchitecture, use of intermediate losses to achieve iterative refinement \nof predictions, masked MSA loss to jointly train with the structure, \nlearning from unlabelled protein sequences using self-distillation and \nself-estimates of accuracy.\nThe AlphaFold network directly predicts the 3D coordinates of all \nheavy atoms for a given protein using the primary amino acid sequence \nand aligned sequences of homologues as inputs (Fig. 1e; see Methods \nfor details of inputs including databases, MSA construction and use of \ntemplates). A description of the most important ideas and components \nis provided below. The full network architecture and training procedure \nare provided in the Supplementary Methods.\nThe network comprises two main stages. First, the trunk of the net-\nwork processes the inputs through repeated layers of a novel neural \nnetwork block that we term Evoformer to produce an Nseq × Nres array \n(Nseq, number of sequences; Nres, number of residues) that represents \na processed MSA and an Nres × Nres array that represents residue pairs. \nThe MSA representation is initialized with the raw MSA (although \nsee Supplementary Methods 1.2.7 for details of handling very deep \nMSAs). The Evoformer blocks contain a number of attention-based \nand non-attention-based components. We show evidence in ‘Interpret-\ning the neural network’ that a concrete structural hypothesis arises \nearly within the Evoformer blocks and is continuously refined. The key \ninnovations in the Evoformer block are new mechanisms to exchange \ninformation within the MSA and pair representations that enable direct \nreasoning about the spatial and evolutionary relationships.\nThe trunk of the network is followed by the structure module that \nintroduces an explicit 3D structure in the form of a rotation and transla-\ntion for each residue of the protein (global rigid body frames). These \nrepresentations are initialized in a trivial state with all rotations set to \nthe identity and all positions set to the origin, but rapidly develop and \nrefine a highly accurate protein structure with precise atomic details. \nKey innovations in this section of the network include breaking the \nchain structure to allow simultaneous local refinement of all parts of \nthe structure, a novel equivariant transformer to allow the network to \nimplicitly reason about the unrepresented side-chain atoms and a loss \nterm that places substantial weight on the orientational correctness \nof the residues. Both within the structure module and throughout \nthe whole network, we reinforce the notion of iterative refinement \nby repeatedly applying the final loss to outputs and then feeding the \noutputs recursively into the same modules. The iterative refinement \nusing the whole network (which we term ‘recycling’ and is related to \napproaches in computer vision28,29) contributes markedly to accuracy \nwith minor extra training time (see Supplementary Methods 1.8 for \ndetails).\nEvoformer\nThe key principle of the building block of the network—named Evo-\nformer (Figs. 1e, 3a)—is to view the prediction of protein structures \nas a graph inference problem in 3D space in which the edges of the \ngraph are defined by residues in proximity. The elements of the pair \nrepresentation encode information about the relation between the \nresidues (Fig. 3b). The columns of the MSA representation encode the  \nindividual residues of the input sequence while the rows represent  \nthe sequences in which those residues appear. Within this framework, \nwe define a number of update operations that are applied in each block \nin which the different update operations are applied in series.\nThe MSA representation updates the pair representation through an \nelement-wise outer product that is summed over the MSA sequence \ndimension. In contrast to previous work30, this operation is applied \nwithin every block rather than once in the network, which enables the \ncontinuous communication from the evolving MSA representation to \nthe pair representation.\nWithin the pair representation, there are two different update pat-\nterns. Both are inspired by the necessity of consistency of the pair \n0–0.5\n0.5–1\n1–2\n2–4\n4–8\n>8\nFull chain Cα r.m.s.d.95 (Å)\n0\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nFraction of proteins\na\n20\n40\n60\n80\n100\nlDDT-Cα of a residue\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFraction of correct F1 rotamers\nb\n0\n20\n40\n60\n80\n100\nAverage pLDDT on the resolved region\n0\n20\n40\n60\n80\n100\nlDDT-Cα\n80\n90\n100\n80\n90\n100\nc\n0\n0.2\n0.4\n0.6\n0.8\n1.0\npTM on the resolved region\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nTM-score\n0.8\n0.9\n1.0\n0.8\n0.9\n1.0\nd\nFig. 2 | Accuracy of AlphaFold on recent PDB structures. The analysed \nstructures are newer than any structure in the training set. Further filtering is \napplied to reduce redundancy (see Methods). a, Histogram of backbone \nr.m.s.d. for full chains (Cα r.m.s.d. at 95% coverage). Error bars are 95% \nconfidence intervals (Poisson). This dataset excludes proteins with a template \n(identified by hmmsearch) from the training set with more than 40% sequence \nidentity covering more than 1% of the chain (n = 3,144 protein chains). The \noverall median is 1.46 Å (95% confidence interval = 1.40–1.56 Å). Note that this \nmeasure will be highly sensitive to domain packing and domain accuracy; a \nhigh r.m.s.d. is expected for some chains with uncertain packing or packing \nerrors. b, Correlation between backbone accuracy and side-chain accuracy. \nFiltered to structures with any observed side chains and resolution better than \n2.5 Å (n = 5,317 protein chains); side chains were further filtered to \nB-factor <30 Å2. A rotamer is classified as correct if the predicted torsion angle \nis within 40°. Each point aggregates a range of lDDT-Cα, with a bin size of 2 units \nabove 70 lDDT-Cα and 5 units otherwise. Points correspond to the mean \naccuracy; error bars are 95% confidence intervals (Student t-test) of the mean \non a per-residue basis. c, Confidence score compared to the true accuracy on \nchains. Least-squares linear fit lDDT-Cα = 0.997 × pLDDT − 1.17 (Pearson’s \nr = 0.76). n = 10,795 protein chains. The shaded region of the linear fit \nrepresents a 95% confidence interval estimated from 10,000 bootstrap \nsamples. In the companion paper39, additional quantification of the reliability \nof pLDDT as a confidence measure is provided. d, Correlation between pTM \nand full chain TM-score. Least-squares linear fit TM-score = 0.98 × pTM + 0.07 \n(Pearson’s r = 0.85). n = 10,795 protein chains. The shaded region of the linear fit \nrepresents a 95% confidence interval estimated from 10,000 bootstrap \nsamples.\n586  |  Nature  |  Vol 596  |  26 August 2021\nArticle\nrepresentation—for a pairwise description of amino acids to be represent-\nable as a single 3D structure, many constraints must be satisfied including \nthe triangle inequality on distances. On the basis of this intuition, we \narrange the update operations on the pair representation in terms of \ntriangles of edges involving three different nodes (Fig. 3c). In particular, \nwe add an extra logit bias to axial attention31 to include the ‘missing edge’ \nof the triangle and we define a non-attention update operation ‘triangle \nmultiplicative update’ that uses two edges to update the missing third \nedge (see Supplementary Methods 1.6.5 for details). The triangle multipli-\ncative update was developed originally as a more symmetric and cheaper \nreplacement for the attention, and networks that use only the attention or \nmultiplicative update are both able to produce high-accuracy structures. \nHowever, the combination of the two updates is more accurate.\nWe also use a variant of axial attention within the MSA representation. \nDuring the per-sequence attention in the MSA, we project additional \nlogits from the pair stack to bias the MSA attention. This closes the loop \nby providing information flow from the pair representation back into \nthe MSA representation, ensuring that the overall Evoformer block is \nable to fully mix information between the pair and MSA representations \nand prepare for structure generation within the structure module.\n \nEnd-to-end structure prediction\nThe structure module (Fig. 3d) operates on a concrete 3D backbone \nstructure using the pair representation and the original sequence row \n(single representation) of the MSA representation from the trunk. The \n3D backbone structure is represented as Nres independent rotations \nand translations, each with respect to the global frame (residue gas) \n(Fig. 3e). These rotations and translations—representing the geometry \nof the N-Cα-C atoms—prioritize the orientation of the protein back-\nbone so that the location of the side chain of each residue is highly \nconstrained within that frame. Conversely, the peptide bond geometry \nis completely unconstrained and the network is observed to frequently \nviolate the chain constraint during the application of the structure mod-\nule as breaking this constraint enables the local refinement of all parts \nof the chain without solving complex loop closure problems. Satisfac-\ntion of the peptide bond geometry is encouraged during fine-tuning \nby a violation loss term. Exact enforcement of peptide bond geometry \nis only achieved in the post-prediction relaxation of the structure by \ngradient descent in the Amber32 force field. Empirically, this final relaxa-\ntion does not improve the accuracy of the model as measured by the \nf\nBackbone frames\n(r, 3×3) and (r,3)\nBackbone frames\n(r, 3×3) and (r,3) \n(initially all at the origin)\nPredict relative \nrotations and \ntranslations\nPredict F angles \nand compute all \natom positions \n(Rk, tk)\nxi\n8 blocks (shared weights)\nd\nPair \nrepresentation\n(r,r,c)\nSingle repr. (r,c) \nSingle repr. (r,c) \nMSA \nrepresentation \n(s,r,c)\nPair \nrepresentation\n(r,r,c)\nRow-wise \ngated \nself-\nattention \nwith pair \nbias\nColumn-\nwise \ngated \nself-\nattention \n+\nTran-\nsition\nOuter \nproduct \nmean\n+\n+\nTriangle \nupdate \nusing \noutgoing\nedges\nTriangle \nself-\nattention \naround \nstarting \nnode\nTriangle \nself-\nattention \naround \nending \nnode\nTriangle \nupdate \nusing \nincoming\nedges\nTran-\nsition\nMSA \nrepresentation \n(s,r,c)\nPair \nrepresentation\n(r,r,c)\na\n+\n+\n+\n+\n+\n+\nTriangle self-attention around \nstarting node\nTriangle self-attention around \nending node\nki\nij\nkj\njk\nij\nik\ni\nk\nj\ni\nj\nk\nki\nij\nkj\ni\nj\nk\nik\nij\njk\ni\nj\nk\nTriangle multiplicative update\nusing ‘outgoing’ edges\nTriangle multiplicative update\nusing ‘incoming’ edges\nc\nIPA\nmodule\n+\nPair representation \n(r,r,c)\nij\ni\nj\nk\nik\ni\nj\nk\njk\nkj\nji\nki\ni\nj\nk\nij\nji\njk\nkj\nki\nik\nCorresponding edges \nin a graph\nb\n48 blocks (no shared weights)\ne\n(Rk, tk)\n~\n~\nxi\n~\nFig. 3 | Architectural details. a, Evoformer block. Arrows show the information \nflow. The shape of the arrays is shown in parentheses. b, The pair representation \ninterpreted as directed edges in a graph. c, Triangle multiplicative update and \ntriangle self-attention. The circles represent residues. Entries in the pair \nrepresentation are illustrated as directed edges and in each diagram, the edge \nbeing updated is ij. d, Structure module including Invariant point attention (IPA) \nmodule. The single representation is a copy of the first row of the MSA \nrepresentation. e, Residue gas: a representation of each residue as one \nfree-floating rigid body for the backbone (blue triangles) and χ angles for the \nside chains (green circles). The corresponding atomic structure is shown below. \nf, Frame aligned point error (FAPE). Green, predicted structure; grey, true \nstructure; (Rk, tk), frames; xi, atom positions.\nNature  |  Vol 596  |  26 August 2021  |  587\nglobal distance test (GDT)33 or lDDT-Cα34 but does remove distracting \nstereochemical violations without the loss of accuracy.\nThe residue gas representation is updated iteratively in two stages \n(Fig. 3d). First, a geometry-aware attention operation that we term \n‘invariant point attention’ (IPA) is used to update an Nres set of neural \nactivations (single representation) without changing the 3D positions, \nthen an equivariant update operation is performed on the residue gas \nusing the updated activations. The IPA augments each of the usual \nattention queries, keys and values with 3D points that are produced \nin the local frame of each residue such that the final value is invariant \nto global rotations and translations (see Methods ‘IPA’ for details). The \n3D queries and keys also impose a strong spatial/locality bias on the \nattention, which is well-suited to the iterative refinement of the protein \nstructure. After each attention operation and element-wise transition \nblock, the module computes an update to the rotation and translation \nof each backbone frame. The application of these updates within the \nlocal frame of each residue makes the overall attention and update \nblock an equivariant operation on the residue gas.\nPredictions of side-chain χ angles as well as the final, per-residue \naccuracy of the structure (pLDDT) are computed with small per-residue \nnetworks on the final activations at the end of the network. The estimate \nof the TM-score (pTM) is obtained from a pairwise error prediction that \nis computed as a linear projection from the final pair representation. The \nfinal loss (which we term the frame-aligned point error (FAPE) (Fig. 3f)) \ncompares the predicted atom positions to the true positions under \nmany different alignments. For each alignment, defined by aligning \nthe predicted frame (Rk, tk) to the corresponding true frame, we com-\npute the distance of all predicted atom positions xi from the true atom \npositions. The resulting Nframes × Natoms distances are penalized with a \nclamped L1 loss. This creates a strong bias for atoms to be correct relative \nto the local frame of each residue and hence correct with respect to its \nside-chain interactions, as well as providing the main source of chirality \nfor AlphaFold (Supplementary Methods 1.9.3 and Supplementary Fig. 9).\nTraining with labelled and unlabelled data\nThe AlphaFold architecture is able to train to high accuracy using only \nsupervised learning on PDB data, but we are able to enhance accuracy \n(Fig. 4a) using an approach similar to noisy student self-distillation35. \nIn this procedure, we use a trained network to predict the structure of \naround 350,000 diverse sequences from Uniclust3036 and make a new \ndataset of predicted structures filtered to a high-confidence subset. We \nthen train the same architecture again from scratch using a mixture of \nPDB data and this new dataset of predicted structures as the training \ndata, in which the various training data augmentations such as crop-\nping and MSA subsampling make it challenging for the network to \nrecapitulate the previously predicted structures. This self-distillation \nprocedure makes effective use of the unlabelled sequence data and \nconsiderably improves the accuracy of the resulting network.\nAdditionally, we randomly mask out or mutate individual residues \nwithin the MSA and have a Bidirectional Encoder Representations from \nTransformers (BERT)-style37 objective to predict the masked elements of \nthe MSA sequences. This objective encourages the network to learn to \ninterpret phylogenetic and covariation relationships without hardcoding \na particular correlation statistic into the features. The BERT objective is \ntrained jointly with the normal PDB structure loss on the same training \nexamples and is not pre-trained, in contrast to recent independent work38.\nInterpreting the neural network\nTo understand how AlphaFold predicts protein structure, we trained \na separate structure module for each of the 48 Evoformer blocks in \nthe network while keeping all parameters of the main network fro-\nzen (Supplementary Methods 1.14). Including our recycling stages, \nthis provides a trajectory of 192 intermediate structures—one per full \nEvoformer block—in which each intermediate represents the belief of \nthe network of the most likely structure at that block. The resulting \ntrajectories are surprisingly smooth after the first few blocks, show-\ning that AlphaFold makes constant incremental improvements to the \nstructure until it can no longer improve (see Fig. 4b for a trajectory of \naccuracy). These trajectories also illustrate the role of network depth. \nFor very challenging proteins such as ORF8 of SARS-CoV-2 (T1064), \nthe network searches and rearranges secondary structure elements \nfor many layers before settling on a good structure. For other proteins \nsuch as LmrP (T1024), the network finds the final structure within the \nfirst few layers. Structure trajectories of CASP14 targets T1024, T1044, \nT1064 and T1091 that demonstrate a clear iterative building process \nfor a range of protein sizes and difficulties are shown in Supplementary \nVideos 1–4. In Supplementary Methods 1.16 and Supplementary Figs. 12, \n13, we interpret the attention maps produced by AlphaFold layers.\nFigure 4a contains detailed ablations of the components of AlphaFold \nthat demonstrate that a variety of different mechanisms contribute \nto AlphaFold accuracy. Detailed descriptions of each ablation model, \ntheir training details, extended discussion of ablation results and the \n–20\n–10\n0\nGDT difference compared\nwith baseline\nNo IPA and no recycling\nNo end-to-end structure gradients\n(keep auxiliary heads)\nNo triangles, biasing or gating\n(use axial attention)\nNo recycling\nNo auxiliary masked MSA head\nNo IPA (use direct projection)\nNo raw MSA\n(use MSA pairwise frequencies)\nNo auxiliary distogram head\nNo templates\nBaseline\nWith self-distillation training\nTest set of CASP14 domains\n–4\n–2\n0\n2\nlDDT-Cα difference\ncompared with baseline\nTest set of PDB chains\na\n0\n48\n96\n144\n192\nEvoformer block\n0\n20\n40\n60\n80\n100\nDomain GDT\nT1024 D1\nT1024 D2\nT1064 D1\nb\nFig. 4 | Interpreting the neural network. a, Ablation results on two target sets: \nthe CASP14 set of domains (n = 87 protein domains) and the PDB test set of \nchains with template coverage of ≤30% at 30% identity (n = 2,261 protein \nchains). Domains are scored with GDT and chains are scored with lDDT-Cα. The \nablations are reported as a difference compared with the average of the three \nbaseline seeds. Means (points) and 95% bootstrap percentile intervals (error \nbars) are computed using bootstrap estimates of 10,000 samples. b, Domain \nGDT trajectory over 4 recycling iterations and 48 Evoformer blocks on CASP14 \ntargets LmrP (T1024) and Orf8 (T1064) where D1 and D2 refer to the individual \ndomains as defined by the CASP assessment. Both T1024 domains obtain the \ncorrect structure early in the network, whereas the structure of T1064 changes \nmultiple times and requires nearly the full depth of the network to reach the \nfinal structure. Note, 48 Evoformer blocks comprise one recycling iteration.\n588  |  Nature  |  Vol 596  |  26 August 2021\nArticle\neffect of MSA depth on each ablation are provided in Supplementary \nMethods 1.13 and Supplementary Fig. 10.\nMSA depth and cross-chain contacts\nAlthough AlphaFold has a high accuracy across the vast majority of \ndeposited PDB structures, we note that there are still factors that affect \naccuracy or limit the applicability of the model. The model uses MSAs \nand the accuracy decreases substantially when the median alignment \ndepth is less than around 30 sequences (see Fig. 5a for details). We \nobserve a threshold effect where improvements in MSA depth over \naround 100 sequences lead to small gains. We hypothesize that the MSA \ninformation is needed to coarsely find the correct structure within the \nearly stages of the network, but refinement of that prediction into a \nhigh-accuracy model does not depend crucially on the MSA information. \nThe other substantial limitation that we have observed is that AlphaFold \nis much weaker for proteins that have few intra-chain or homotypic con-\ntacts compared to the number of heterotypic contacts (further details \nare provided in a companion paper39). This typically occurs for bridging \ndomains within larger complexes in which the shape of the protein is \ncreated almost entirely by interactions with other chains in the complex. \nConversely, AlphaFold is often able to give high-accuracy predictions for \nhomomers, even when the chains are substantially intertwined (Fig. 5b). \nWe expect that the ideas of AlphaFold are readily applicable to predicting \nfull hetero-complexes in a future system and that this will remove the dif-\nficulty with protein chains that have a large number of hetero-contacts.\nRelated work\nThe prediction of protein structures has had a long and varied develop-\nment, which is extensively covered in a number of reviews14,40–43. Despite \nthe long history of applying neural networks to structure prediction14,42,43, \nthey have only recently come to improve structure prediction10,11,44,45. \nThese approaches effectively leverage the rapid improvement in com-\nputer vision systems46 by treating the problem of protein structure \nprediction as converting an ‘image’ of evolutionary couplings22–24 to an \n‘image’ of the protein distance matrix and then integrating the distance \npredictions into a heuristic system that produces the final 3D coordinate \nprediction. A few recent studies have been developed to predict the 3D \ncoordinates directly47–50, but the accuracy of these approaches does not \nmatch traditional, hand-crafted structure prediction pipelines51. In paral-\nlel, the success of attention-based networks for language processing52 \nand, more recently, computer vision31,53 has inspired the exploration of \nattention-based methods for interpreting protein sequences54–56.\nDiscussion\nThe methodology that we have taken in designing AlphaFold is a combi-\nnation of the bioinformatics and physical approaches: we use a physical \nand geometric inductive bias to build components that learn from PDB \ndata with minimal imposition of handcrafted features (for example, \nAlphaFold builds hydrogen bonds effectively without a hydrogen bond \nscore function). This results in a network that learns far more efficiently \nfrom the limited data in the PDB but is able to cope with the complexity \nand variety of structural data.\nIn particular, AlphaFold is able to handle missing the physical context \nand produce accurate models in challenging cases such as intertwined \nhomomers or proteins that only fold in the presence of an unknown \nhaem group. The ability to handle underspecified structural conditions \nis essential to learning from PDB structures as the PDB represents the \nfull range of conditions in which structures have been solved. In gen-\neral, AlphaFold is trained to produce the protein structure most likely \nto appear as part of a PDB structure. For example, in cases in which a \nparticular stochiometry, ligand or ion is predictable from the sequence \nalone, AlphaFold is likely to produce a structure that respects those \nconstraints implicitly.\nAlphaFold has already demonstrated its utility to the experimental \ncommunity, both for molecular replacement57 and for interpreting \ncryogenic electron microscopy maps58. Moreover, because AlphaFold \noutputs protein coordinates directly, AlphaFold produces predictions \nin graphics processing unit (GPU) minutes to GPU hours depending on \nthe length of the protein sequence (for example, around one GPU min-\nute per model for 384 residues; see Methods for details). This opens up \nthe exciting possibility of predicting structures at the proteome-scale \nand beyond—in a companion paper39, we demonstrate the application \nof AlphaFold to the entire human proteome39.\nThe explosion in available genomic sequencing techniques and data \nhas revolutionized bioinformatics but the intrinsic challenge of experi-\nmental structure determination has prevented a similar expansion in \nour structural knowledge. By developing an accurate protein structure \n100\n101\n102\n103\n104\nMedian per-residue Neff for the chain\n0\n20\n40\n60\n80\n100\nlDDT-Cα\nCoverage < 0.3\nCoverage > 0.6\na\nAlphaFold Experiment\nb\nFig. 5 | Effect of MSA depth and cross-chain contacts. a, Backbone accuracy \n(lDDT-Cα) for the redundancy-reduced set of the PDB after our training data \ncut-off, restricting to proteins in which at most 25% of the long-range contacts \nare between different heteromer chains. We further consider two groups of \nproteins based on template coverage at 30% sequence identity: covering more \nthan 60% of the chain (n = 6,743 protein chains) and covering less than 30% of \nthe chain (n = 1,596 protein chains). MSA depth is computed by counting the \nnumber of non-gap residues for each position in the MSA (using the Neff \nweighting scheme; see Methods for details) and taking the median across \nresidues. The curves are obtained through Gaussian kernel average smoothing \n(window size is 0.2 units in log10(Neff)); the shaded area is the 95% confidence \ninterval estimated using bootstrap of 10,000 samples. b, An intertwined \nhomotrimer (PDB 6SK0) is correctly predicted without input stoichiometry \nand only a weak template (blue is predicted and green is experimental).\nNature  |  Vol 596  |  26 August 2021  |  589\nprediction algorithm, coupled with existing large and well-curated \nstructure and sequence databases assembled by the experimental \ncommunity, we hope to accelerate the advancement of structural \nbioinformatics that can keep pace with the genomics revolution. We \nhope that AlphaFold—and computational approaches that apply its \ntechniques for other biophysical problems—will become essential \ntools of modern biology.\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author con-\ntributions and competing interests; and statements of data and code \navailability are available at https://doi.org/10.1038/s41586-021-03819-2.\n1.\t\nThompson, M. C., Yeates, T. O. & Rodriguez, J. A. Advances in methods for atomic \nresolution macromolecular structure determination. F1000Res. 9, 667 (2020).\n2.\t\nBai, X.-C., McMullan, G. & Scheres, S. H. W. How cryo-EM is revolutionizing structural \nbiology. Trends Biochem. Sci. 40, 49–57 (2015).\n3.\t\nJaskolski, M., Dauter, Z. & Wlodawer, A. A brief history of macromolecular crystallography, \nillustrated by a family tree and its Nobel fruits. FEBS J. 281, 3985–4009 (2014).\n4.\t\nWüthrich, K. The way to NMR structures of proteins. Nat. Struct. Biol. 8, 923–925 (2001).\n5.\t\nwwPDB Consortium. Protein Data Bank: the single global archive for 3D macromolecular \nstructure data. Nucleic Acids Res. 47, D520–D528 (2018).\n6.\t\nMitchell, A. L. et al. MGnify: the microbiome analysis resource in 2020. Nucleic Acids Res. \n48, D570–D578 (2020).\n7.\t\nSteinegger, M., Mirdita, M. & Söding, J. Protein-level assembly increases protein sequence \nrecovery from metagenomic samples manyfold. Nat. Methods 16, 603–606 (2019).\n8.\t\nDill, K. A., Ozkan, S. B., Shell, M. S. & Weikl, T. R. The protein folding problem. Annu. Rev. \nBiophys. 37, 289–316 (2008).\n9.\t\nAnfinsen, C. B. Principles that govern the folding of protein chains. Science 181, 223–230 \n(1973).\n10.\t\nSenior, A. W. et al. Improved protein structure prediction using potentials from deep \nlearning. Nature 577, 706–710 (2020).\n11.\t\nWang, S., Sun, S., Li, Z., Zhang, R. & Xu, J. Accurate de novo prediction of protein contact \nmap by ultra-deep learning model. PLOS Comput. Biol. 13, e1005324 (2017).\n12.\t\nZheng, W. et al. Deep-learning contact-map guided protein structure prediction in \nCASP13. Proteins 87, 1149–1164 (2019).\n13.\t\nAbriata, L. A., Tamò, G. E. & Dal Peraro, M. A further leap of improvement in tertiary \nstructure prediction in CASP13 prompts new routes for future assessments. Proteins 87, \n1100–1112 (2019).\n14.\t\nPearce, R. & Zhang, Y. Deep learning techniques have significantly impacted protein \nstructure prediction and protein design. Curr. Opin. Struct. Biol. 68, 194–207 (2021).\n15.\t\nMoult, J., Fidelis, K., Kryshtafovych, A., Schwede, T. & Topf, M. Critical assessment of \ntechniques for protein structure prediction, fourteenth round. CASP 14 Abstract Book \nhttps://www.predictioncenter.org/casp14/doc/CASP14_Abstracts.pdf (2020).\n16.\t\nBrini, E., Simmerling, C. & Dill, K. Protein storytelling through physics. Science 370, \neaaz3041 (2020).\n17.\t\nSippl, M. J. Calculation of conformational ensembles from potentials of mean force.  \nAn approach to the knowledge-based prediction of local structures in globular proteins. \nJ. Mol. Biol. 213, 859–883 (1990).\n18.\t\nŠali, A. & Blundell, T. L. Comparative protein modelling by satisfaction of spatial \nrestraints. J. Mol. Biol. 234, 779–815 (1993).\n19.\t\nRoy, A., Kucukural, A. & Zhang, Y. I-TASSER: a unified platform for automated protein \nstructure and function prediction. Nat. Protocols 5, 725–738 (2010).\n20.\t Altschuh, D., Lesk, A. M., Bloomer, A. C. & Klug, A. Correlation of co-ordinated amino acid \nsubstitutions with function in viruses related to tobacco mosaic virus. J. Mol. Biol. 193, \n693–707 (1987).\n21.\t\nShindyalov, I. N., Kolchanov, N. A. & Sander, C. Can three-dimensional contacts in protein \nstructures be predicted by analysis of correlated mutations? Protein Eng. 7, 349–358 (1994).\n22.\t Weigt, M., White, R. A., Szurmant, H., Hoch, J. A. & Hwa, T. Identification of direct residue \ncontacts in protein–protein interaction by message passing. Proc. Natl Acad. Sci. USA \n106, 67–72 (2009).\n23.\t Marks, D. S. et al. Protein 3D structure computed from evolutionary sequence variation. \nPLoS ONE 6, e28766 (2011).\n24.\t Jones, D. T., Buchan, D. W. A., Cozzetto, D. & Pontil, M. PSICOV: precise structural contact \nprediction using sparse inverse covariance estimation on large multiple sequence \nalignments. Bioinformatics 28, 184–190 (2012).\n25.\t Moult, J., Pedersen, J. T., Judson, R. & Fidelis, K. A large-scale experiment to assess protein \nstructure prediction methods. Proteins 23, ii–iv (1995).\n26.\t Kryshtafovych, A., Schwede, T., Topf, M., Fidelis, K. & Moult, J. Critical assessment of \nmethods of protein structure prediction (CASP)-round XIII. Proteins 87, 1011–1020 (2019).\n27.\t\nZhang, Y. & Skolnick, J. Scoring function for automated assessment of protein structure \ntemplate quality. Proteins 57, 702–710 (2004).\n28.\t Tu, Z. & Bai, X. Auto-context and its application to high-level vision tasks and 3D brain \nimage segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 32, 1744–1757 (2010).\n29.\t Carreira, J., Agrawal, P., Fragkiadaki, K. & Malik, J. Human pose estimation with iterative \nerror feedback. In Proc. IEEE Conference on Computer Vision and Pattern Recognition \n4733–4742 (2016).\n30.\t Mirabello, C. & Wallner, B. rawMSA: end-to-end deep learning using raw multiple \nsequence alignments. PLoS ONE 14, e0220182 (2019).\n31.\t\nHuang, Z. et al. CCNet: criss-cross attention for semantic segmentation. In Proc. IEEE/CVF \nInternational Conference on Computer Vision 603–612 (2019).\n32.\t Hornak, V. et al. Comparison of multiple Amber force fields and development of \nimproved protein backbone parameters. Proteins 65, 712–725 (2006).\n33.\t Zemla, A. LGA: a method for finding 3D similarities in protein structures. Nucleic Acids \nRes. 31, 3370–3374 (2003).\n34.\t Mariani, V., Biasini, M., Barbato, A. & Schwede, T. lDDT: a local superposition-free score for \ncomparing protein structures and models using distance difference tests. Bioinformatics \n29, 2722–2728 (2013).\n35.\t Xie, Q., Luong, M.-T., Hovy, E. & Le, Q. V. Self-training with noisy student improves \nimagenet classification. In Proc. IEEE/CVF Conference on Computer Vision and Pattern \nRecognition 10687–10698 (2020).\n36.\t Mirdita, M. et al. Uniclust databases of clustered and deeply annotated protein \nsequences and alignments. Nucleic Acids Res. 45, D170–D176 (2017).\n37.\t\nDevlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training of deep bidirectional \ntransformers for language understanding. In Proc. 2019 Conference of the North \nAmerican Chapter of the Association for Computational Linguistics: Human Language \nTechnologies 1, 4171–4186 (2019).\n38.\t Rao, R. et al. MSA transformer. In Proc. 38th International Conference on Machine \nLearning PMLR 139, 8844–8856 (2021).\n39.\t Tunyasuvunakool, K. et al. Highly accurate protein structure prediction for the human \nproteome. Nature https://doi.org/10.1038/s41586-021-03828-1 (2021).\n40.\t Kuhlman, B. & Bradley, P. Advances in protein structure prediction and design. Nat. Rev. \nMol. Cell Biol. 20, 681–697 (2019).\n41.\t\nMarks, D. S., Hopf, T. A. & Sander, C. Protein structure prediction from sequence variation. \nNat. Biotechnol. 30, 1072–1080 (2012).\n42.\t Qian, N. & Sejnowski, T. J. Predicting the secondary structure of globular proteins using \nneural network models. J. Mol. Biol. 202, 865–884 (1988).\n43.\t Fariselli, P., Olmea, O., Valencia, A. & Casadio, R. Prediction of contact maps with neural \nnetworks and correlated mutations. Protein Eng. 14, 835–843 (2001).\n44.\t Yang, J. et al. Improved protein structure prediction using predicted interresidue \norientations. Proc. Natl Acad. Sci. USA 117, 1496–1503 (2020).\n45.\t Li, Y. et al. Deducing high-accuracy protein contact-maps from a triplet of coevolutionary \nmatrices through deep residual convolutional networks. PLOS Comput. Biol. 17, \ne1008865 (2021).\n46.\t He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In  \nProc. IEEE Conference on Computer Vision and Pattern Recognition 770–778 (2016).\n47.\t\nAlQuraishi, M. End-to-end differentiable learning of protein structure. Cell Syst. 8,  \n292–301 (2019).\n48.\t Senior, A. W. et al. Protein structure prediction using multiple deep neural networks in the \n13th Critical Assessment of Protein Structure Prediction (CASP13). Proteins 87, 1141–1148 \n(2019).\n49.\t Ingraham, J., Riesselman, A. J., Sander, C. & Marks, D. S. Learning protein structure with a \ndifferentiable simulator. in Proc. International Conference on Learning Representations \n(2019).\n50.\t Li, J. Universal transforming geometric network. Preprint at https://arxiv.org/\nabs/1908.00723 (2019).\n51.\t\nXu, J., McPartlon, M. & Li, J. Improved protein structure prediction by deep learning \nirrespective of co-evolution information. Nat. Mach. Intell. 3, 601–609 (2021).\n52.\t Vaswani, A. et al. Attention is all you need. In Advances in Neural Information Processing \nSystems 5998–6008 (2017).\n53.\t Wang, H. et al. Axial-deeplab: stand-alone axial-attention for panoptic segmentation. in \nEuropean Conference on Computer Vision 108–126 (Springer, 2020).\n54.\t Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. & Church, G. M. Unified rational \nprotein engineering with sequence-based deep representation learning. Nat. Methods 16, \n1315–1322 (2019).\n55.\t Heinzinger, M. et al. Modeling aspects of the language of life through transfer-learning \nprotein sequences. BMC Bioinformatics 20, 723 (2019).\n56.\t Rives, A. et al. Biological structure and function emerge from scaling unsupervised \nlearning to 250 million protein sequences. Proc. Natl Acad. Sci. USA 118, e2016239118 \n(2021).\n57.\t\nPereira, J. et al. High-accuracy protein structure prediction in CASP14. Proteins https://doi.\norg/10.1002/prot.26171 (2021).\n58.\t Gupta, M. et al. CryoEM and AI reveal a structure of SARS-CoV-2 Nsp2, a multifunctional \nprotein involved in key host processes. Preprint at https://doi.org/10.1101/2021.05.10. \n443524 (2021).\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution \n4.0 International License, which permits use, sharing, adaptation, distribution \nand reproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the Creative Commons license, \nand indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons license, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons license and your \nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this license, \nvisit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2021\nArticle\nMethods\nFull algorithm details\nExtensive explanations of the components and their motivations are \navailable in Supplementary Methods 1.1–1.10, in addition, pseudocode \nis available in Supplementary Information Algorithms 1–32, network \ndiagrams in Supplementary Figs. 1–8, input features in Supplementary \nTable 1 and additional details are provided in Supplementary Tables 2, 3. \nTraining and inference details are provided in Supplementary Methods \n1.11–1.12 and Supplementary Tables 4, 5.\nIPA\nThe IPA module combines the pair representation, the single repre-\nsentation and the geometric representation to update the single rep-\nresentation (Supplementary Fig. 8). Each of these representations \ncontributes affinities to the shared attention weights and then uses \nthese weights to map its values to the output. The IPA operates in 3D \nspace. Each residue produces query points, key points and value points \nin its local frame. These points are projected into the global frame using \nthe backbone frame of the residue in which they interact with each \nother. The resulting points are then projected back into the local frame. \nThe affinity computation in the 3D space uses squared distances and \nthe coordinate transformations ensure the invariance of this module \nwith respect to the global frame (see Supplementary Methods 1.8.2 \n‘Invariant point attention (IPA)’ for the algorithm, proof of invariance \nand a description of the full multi-head version). A related construc-\ntion that uses classic geometric invariants to construct pairwise fea-\ntures in place of the learned 3D points has been applied to protein  \ndesign59.\nIn addition to the IPA, standard dot product attention is computed \non the abstract single representation and a special attention on the pair \nrepresentation. The pair representation augments both the logits and \nthe values of the attention process, which is the primary way in which \nthe pair representation controls the structure generation.\nInputs and data sources\nInputs to the network are the primary sequence, sequences from evo-\nlutionarily related proteins in the form of a MSA created by standard \ntools including jackhmmer60 and HHBlits61, and 3D atom coordinates \nof a small number of homologous structures (templates) where avail-\nable. For both the MSA and templates, the search processes are tuned \nfor high recall; spurious matches will probably appear in the raw MSA \nbut this matches the training condition of the network.\nOne of the sequence databases used, Big Fantastic Database (BFD), \nwas custom-made and released publicly (see ‘Data availability’) and \nwas used by several CASP teams. BFD is one of the largest publicly avail-\nable collections of protein families. It consists of 65,983,866 families \nrepresented as MSAs and hidden Markov models (HMMs) covering \n2,204,359,010 protein sequences from reference databases, metage-\nnomes and metatranscriptomes.\nBFD was built in three steps. First, 2,423,213,294 protein sequences \nwere collected from UniProt (Swiss-Prot&TrEMBL, 2017-11)62, a soil refer-\nence protein catalogue and the marine eukaryotic reference catalogue7, \nand clustered to 30% sequence identity, while enforcing a 90% align-\nment coverage of the shorter sequences using MMseqs2/Linclust63. \nThis resulted in 345,159,030 clusters. For computational efficiency, \nwe removed all clusters with less than three members, resulting in \n61,083,719 clusters. Second, we added 166,510,624 representative pro-\ntein sequences from Metaclust NR (2017-05; discarding all sequences \nshorter than 150 residues)63 by aligning them against the cluster rep-\nresentatives using MMseqs264. Sequences that fulfilled the sequence \nidentity and coverage criteria were assigned to the best scoring cluster. \nThe remaining 25,347,429 sequences that could not be assigned were \nclustered separately and added as new clusters, resulting in the final \nclustering. Third, for each of the clusters, we computed an MSA using \nFAMSA65 and computed the HMMs following the Uniclust HH-suite \ndatabase protocol36.\nThe following versions of public datasets were used in this study. Our \nmodels were trained on a copy of the PDB5 downloaded on 28 August \n2019. For finding template structures at prediction time, we used a copy \nof the PDB downloaded on 14 May 2020, and the PDB7066 clustering \ndatabase downloaded on 13 May 2020. For MSA lookup at both training \nand prediction time, we used Uniref9067 v.2020_01, BFD, Uniclust3036 \nv.2018_08 and MGnify6 v.2018_12. For sequence distillation, we used \nUniclust3036 v.2018_08 to construct a distillation structure dataset. \nFull details are provided in Supplementary Methods 1.2.\nFor MSA search on BFD + Uniclust30, and template search against \nPDB70, we used HHBlits61 and HHSearch66 from hh-suite v.3.0-beta.3 \n(version 14/07/2017). For MSA search on Uniref90 and clustered MGnify, \nwe used jackhmmer from HMMER368. For constrained relaxation of \nstructures, we used OpenMM v.7.3.169 with the Amber99sb force field32. \nFor neural network construction, running and other analyses, we used \nTensorFlow70, Sonnet71, NumPy72, Python73 and Colab74.\nTo quantify the effect of the different sequence data sources, we \nre-ran the CASP14 proteins using the same models but varying how the \nMSA was constructed. Removing BFD reduced the mean accuracy by \n0.4 GDT, removing Mgnify reduced the mean accuracy by 0.7 GDT, and \nremoving both reduced the mean accuracy by 6.1 GDT. In each case, we \nfound that most targets had very small changes in accuracy but a few \noutliers had very large (20+ GDT) differences. This is consistent with the \nresults in Fig. 5a in which the depth of the MSA is relatively unimportant \nuntil it approaches a threshold value of around 30 sequences when the \nMSA size effects become quite large. We observe mostly overlapping \neffects between inclusion of BFD and Mgnify, but having at least one \nof these metagenomics databases is very important for target classes \nthat are poorly represented in UniRef, and having both was necessary \nto achieve full CASP accuracy.\nTraining regimen\nTo train, we use structures from the PDB with a maximum release date \nof 30 April 2018. Chains are sampled in inverse proportion to cluster \nsize of a 40% sequence identity clustering. We then randomly crop \nthem to 256 residues and assemble into batches of size 128. We train the \nmodel on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU \ncore, hence the model uses 128 TPU v3 cores. The model is trained until \nconvergence (around 10 million samples) and further fine-tuned using \nlonger crops of 384 residues, larger MSA stack and reduced learning \nrate (see Supplementary Methods 1.11 for the exact configuration). The \ninitial training stage takes approximately 1 week, and the fine-tuning \nstage takes approximately 4 additional days.\nThe network is supervised by the FAPE loss and a number of auxil-\niary losses. First, the final pair representation is linearly projected to \na binned distance distribution (distogram) prediction, scored with \na cross-entropy loss. Second, we use random masking on the input \nMSAs and require the network to reconstruct the masked regions \nfrom the output MSA representation using a BERT-like loss37. Third, \nthe output single representations of the structure module are used to \npredict binned per-residue lDDT-Cα values. Finally, we use an auxiliary \nside-chain loss during training, and an auxiliary structure violation loss \nduring fine-tuning. Detailed descriptions and weighting are provided \nin the Supplementary Information.\nAn initial model trained with the above objectives was used to make \nstructure predictions for a Uniclust dataset of 355,993 sequences with \nthe full MSAs. These predictions were then used to train a final model \nwith identical hyperparameters, except for sampling examples 75% of \nthe time from the Uniclust prediction set, with sub-sampled MSAs, and \n25% of the time from the clustered PDB set.\nWe train five different models using different random seeds, some \nwith templates and some without, to encourage diversity in the predic-\ntions (see Supplementary Table 5 and Supplementary Methods 1.12.1 \nfor details). We also fine-tuned these models after CASP14 to add a \npTM prediction objective (Supplementary Methods 1.9.7) and use the \nobtained models for Fig. 2d.\nInference regimen\nWe inference the five trained models and use the predicted confidence \nscore to select the best model per target.\nUsing our CASP14 configuration for AlphaFold, the trunk of the net-\nwork is run multiple times with different random choices for the MSA \ncluster centres (see Supplementary Methods 1.11.2 for details of the \nensembling procedure). The full time to make a structure prediction \nvaries considerably depending on the length of the protein. Repre-\nsentative timings for the neural network using a single model on V100 \nGPU are 4.8 min with 256 residues, 9.2 min with 384 residues and 18 h \nat 2,500 residues. These timings are measured using our open-source \ncode, and the open-source code is notably faster than the version we \nran in CASP14 as we now use the XLA compiler75.\nSince CASP14, we have found that the accuracy of the network with-\nout ensembling is very close or equal to the accuracy with ensembling \nand we turn off ensembling for most inference. Without ensembling, \nthe network is 8× faster and the representative timings for a single \nmodel are 0.6 min with 256 residues, 1.1 min with 384 residues and \n2.1 h with 2,500 residues.\nInferencing large proteins can easily exceed the memory of a single \nGPU. For a V100 with 16 GB of memory, we can predict the structure \nof proteins up to around 1,300 residues without ensembling and the \n256- and 384-residue inference times are using the memory of a single \nGPU. The memory usage is approximately quadratic in the number of \nresidues, so a 2,500-residue protein involves using unified memory so \nthat we can greatly exceed the memory of a single V100. In our cloud \nsetup, a single V100 is used for computation on a 2,500-residue protein \nbut we requested four GPUs to have sufficient memory.\nSearching genetic sequence databases to prepare inputs and final \nrelaxation of the structures take additional central processing unit \n(CPU) time but do not require a GPU or TPU.\nMetrics\nThe predicted structure is compared to the true structure from the \nPDB in terms of lDDT metric34, as this metric reports the domain accu-\nracy without requiring a domain segmentation of chain structures. \nThe distances are either computed between all heavy atoms (lDDT) \nor only the Cα atoms to measure the backbone accuracy (lDDT-Cα). \nAs lDDT-Cα only focuses on the Cα atoms, it does not include the pen-\nalty for structural violations and clashes. Domain accuracies in CASP \nare reported as GDT33 and the TM-score27 is used as a full chain global \nsuperposition metric.\nWe also report accuracies using the r.m.s.d.95 (Cα r.m.s.d. at 95% cov-\nerage). We perform five iterations of (1) a least-squares alignment of the \npredicted structure and the PDB structure on the currently chosen Cα \natoms (using all Cα atoms in the first iteration); (2) selecting the 95% \nof Cα atoms with the lowest alignment error. The r.m.s.d. of the atoms \nchosen for the final iterations is the r.m.s.d.95. This metric is more robust \nto apparent errors that can originate from crystal structure artefacts, \nalthough in some cases the removed 5% of residues will contain genuine \nmodelling errors.\nTest set of recent PDB sequences\nFor evaluation on recent PDB sequences (Figs. 2a–d, 4a, 5a), we used \na copy of the PDB downloaded 15 February 2021. Structures were fil-\ntered to those with a release date after 30 April 2018 (the date limit for \ninclusion in the training set for AlphaFold). Chains were further filtered \nto remove sequences that consisted of a single amino acid as well as \nsequences with an ambiguous chemical component at any residue \nposition. Exact duplicates were removed, with the chain with the most \nresolved Cα atoms used as the representative sequence. Subsequently, \nstructures with less than 16 resolved residues, with unknown residues \nor solved by NMR methods were removed. As the PDB contains many \nnear-duplicate sequences, the chain with the highest resolution was \nselected from each cluster in the PDB 40% sequence clustering of the \ndata. Furthermore, we removed all sequences for which fewer than  \n80 amino acids had the alpha carbon resolved and removed chains with \nmore than 1,400 residues. The final dataset contained 10,795 protein \nsequences.\nThe procedure for filtering the recent PDB dataset based on prior \ntemplate identity was as follows. Hmmsearch was run with default \nparameters against a copy of the PDB SEQRES fasta downloaded  \n15 February 2021. Template hits were accepted if the associated struc-\nture had a release date earlier than 30 April 2018. Each residue position \nin a query sequence was assigned the maximum identity of any template \nhit covering that position. Filtering then proceeded as described in \nthe individual figure legends, based on a combination of maximum \nidentity and sequence coverage.\nThe MSA depth analysis was based on computing the normalized \nnumber of effective sequences (Neff) for each position of a query \nsequence. Per-residue Neff values were obtained by counting the num-\nber of non-gap residues in the MSA for this position and weighting the \nsequences using the Neff scheme76 with a threshold of 80% sequence \nidentity measured on the region that is non-gap in either sequence.\nReporting summary\nFurther information on research design is available in the Nature \nResearch Reporting Summary linked to this paper.\nData availability\nAll input data are freely available from public sources.\nStructures from the PDB were used for training and as templates \n(https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated \nsequence data and 40% sequence clustering see also https://ftp.wwpdb.\norg/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/\nsequence/clusters/bc-40.out). Training used a version of the PDB \ndownloaded 28 August 2019, while the CASP14 template search used \na version downloaded 14 May 2020. The template search also used the \nPDB70 database, downloaded 13 May 2020 (https://wwwuser.gwdg.\nde/~compbiol/data/hhsuite/databases/hhsuite_dbs/).\nWe show experimental structures from the PDB with accession num-\nbers 6Y4F77, 6YJ178, 6VR479, 6SK080, 6FES81, 6W6W82, 6T1Z83 and 7JTL84.\nFor MSA lookup at both the training and prediction time, we used \nUniRef90 v.2020_01 (https://ftp.ebi.ac.uk/pub/databases/uniprot/\nprevious_releases/release-2020_01/uniref/), BFD (https://bfd.mmseqs.\ncom), Uniclust30 v.2018_08 (https://wwwuser.gwdg.de/~compbiol/\nuniclust/2018_08/) and MGnify clusters v.2018_12 (https://ftp.ebi.ac.uk/\npub/databases/metagenomics/peptide_database/2018_12/). Uniclust30 \nv.2018_08 was also used as input for constructing a distillation structure \ndataset.\nCode availability\nSource code for the AlphaFold model, trained weights and inference \nscript are available under an open-source license at https://github.\ncom/deepmind/alphafold.\nNeural networks were developed with TensorFlow v.1 (https://github.\ncom/tensorflow/tensorflow), Sonnet v.1 (https://github.com/deep-\nmind/sonnet), JAX v.0.1.69 (https://github.com/google/jax/) and Haiku \nv.0.0.4 (https://github.com/deepmind/dm-haiku). The XLA compiler is \nbundled with JAX and does not have a separate version number.\nFor MSA search on BFD+Uniclust30, and for template search against \nPDB70, we used HHBlits and HHSearch from hh-suite v.3.0-beta.3 \nrelease 14/07/2017 (https://github.com/soedinglab/hh-suite). For MSA \nsearch on UniRef90 and clustered MGnify, we used jackhmmer from \nArticle\nHMMER v.3.3 (http://eddylab.org/software/hmmer/). For constrained \nrelaxation of structures, we used OpenMM v.7.3.1 (https://github.com/\nopenmm/openmm) with the Amber99sb force field.\nConstruction of BFD used MMseqs2 v.925AF (https://github.\ncom/soedinglab/MMseqs2) and FAMSA v.1.2.5 (https://github.com/\nrefresh-bio/FAMSA).\nData analysis used Python v.3.6 (https://www.python.org/), NumPy \nv.1.16.4 (https://github.com/numpy/numpy), SciPy v.1.2.1 (https://www.\nscipy.org/), seaborn v.0.11.1 (https://github.com/mwaskom/seaborn), \nMatplotlib v.3.3.4 (https://github.com/matplotlib/matplotlib), bokeh \nv.1.4.0 (https://github.com/bokeh/bokeh), pandas v.1.1.5 (https://\ngithub.com/pandas-dev/pandas), plotnine v.0.8.0 (https://github.\ncom/has2k1/plotnine), statsmodels v.0.12.2 (https://github.com/\nstatsmodels/statsmodels) and Colab (https://research.google.com/\ncolaboratory). TM-align v.20190822 (https://zhanglab.dcmb.med.\numich.edu/TM-align/) was used for computing TM-scores. Structure \nvisualizations were created in Pymol v.2.3.0 (https://github.com/schro-\ndinger/pymol-open-source).\n \n59.\t Ingraham, J., Garg, V. K., Barzilay, R. & Jaakkola, T. Generative models for graph-based \nprotein design. in Proc. 33rd Conference on Neural Information Processing Systems \n(2019).\n60.\t Johnson, L. S., Eddy, S. R. & Portugaly, E. Hidden Markov model speed heuristic and \niterative HMM search procedure. BMC Bioinformatics 11, 431 (2010).\n61.\t\nRemmert, M., Biegert, A., Hauser, A. & Söding, J. HHblits: lightning-fast iterative protein \nsequence searching by HMM-HMM alignment. Nat. Methods 9, 173–175 (2012).\n62.\t The UniProt Consortium. UniProt: the universal protein knowledgebase in 2021. Nucleic \nAcids Res. 49, D480–D489 (2020).\n63.\t Steinegger, M. & Söding, J. Clustering huge protein sequence sets in linear time. Nat. \nCommun. 9, 2542 (2018).\n64.\t Steinegger, M. & Söding, J. MMseqs2 enables sensitive protein sequence searching for \nthe analysis of massive data sets. Nat. Biotechnol. 35, 1026–1028 (2017).\n65.\t Deorowicz, S., Debudaj-Grabysz, A. & Gudyś, A. FAMSA: fast and accurate multiple \nsequence alignment of huge protein families. Sci. Rep. 6, 33964 (2016).\n66.\t Steinegger, M. et al. HH-suite3 for fast remote homology detection and deep protein \nannotation. BMC Bioinformatics 20, 473 (2019).\n67.\t\nSuzek, B. E., Wang, Y., Huang, H., McGarvey, P. B. & Wu, C. H. UniRef clusters: a \ncomprehensive and scalable alternative for improving sequence similarity searches. \nBioinformatics 31, 926–932 (2015).\n68.\t Eddy, S. R. Accelerated profile HMM searches. PLOS Comput. Biol. 7, e1002195 (2011).\n69.\t Eastman, P. et al. OpenMM 7: rapid development of high performance algorithms for \nmolecular dynamics. PLOS Comput. Biol. 13, e1005659 (2017).\n70.\t Ashish, A. M. A. et al. TensorFlow: large-scale machine learning on heterogeneous \nsystems. Preprint at https://arxiv.org/abs/1603.04467 (2015).\n71.\t\nReynolds, M. et al. Open sourcing Sonnet – a new library for constructing neural \nnetworks. DeepMind https://deepmind.com/blog/open-sourcing-sonnet/ (7 April 2017).\n72.\t Harris, C. R. et al. Array programming with NumPy. Nature 585, 357–362 (2020).\n73.\t Van Rossum, G. & Drake, F. L. Python 3 Reference Manual (CreateSpace, 2009).\n74.\t Bisong, E. in Building Machine Learning and Deep Learning Models on Google Cloud \nPlatform: A Comprehensive Guide for Beginners 59–64 (Apress, 2019).\n75.\t TensorFlow. XLA: Optimizing Compiler for TensorFlow. https://www.tensorflow.org/xla \n(2018).\n76.\t Wu, T., Hou, J., Adhikari, B. & Cheng, J. Analysis of several key factors influencing deep \nlearning-based inter-residue contact prediction. Bioinformatics 36, 1091–1098 (2020).\n77.\t\nJiang, W. et al. MrpH, a new class of metal-binding adhesin, requires zinc to mediate \nbiofilm formation. PLoS Pathog. 16, e1008707 (2020).\n78.\t Dunne, M., Ernst, P., Sobieraj, A., Pluckthun, A. & Loessner, M. J. The M23 peptidase \ndomain of the Staphylococcal phage 2638A endolysin. PDB https://doi.org/10.2210/\npdb6YJ1/pdb (2020).\n79.\t Drobysheva, A. V. et al. Structure and function of virion RNA polymerase of a crAss-like \nphage. Nature 589, 306–309 (2021).\n80.\t Flaugnatti, N. et al. Structural basis for loading and inhibition of a bacterial T6SS \nphospholipase effector by the VgrG spike. EMBO J. 39, e104129 (2020).\n81.\t\nElGamacy, M. et al. An interface-driven design strategy yields a novel, corrugated protein \narchitecture. ACS Synth. Biol. 7, 2226–2235 (2018).\n82.\t Lim, C. J. et al. The structure of human CST reveals a decameric assembly bound to \ntelomeric DNA. Science 368, 1081–1085 (2020).\n83.\t Debruycker, V. et al. An embedded lipid in the multidrug transporter LmrP suggests a \nmechanism for polyspecificity. Nat. Struct. Mol. Biol. 27, 829–835 (2020).\n84.\t Flower, T. G. et al. Structure of SARS-CoV-2 ORF8, a rapidly evolving immune evasion \nprotein. Proc. Natl Acad. Sci. USA 118, e2021785118 (2021).\nAcknowledgements We thank A. Rrustemi, A. Gu, A. Guseynov, B. Hechtman, C. Beattie,  \nC. Jones, C. Donner, E. Parisotto, E. Elsen, F. Popovici, G. Necula, H. Maclean, J. Menick,  \nJ. Kirkpatrick, J. Molloy, J. Yim, J. Stanway, K. Simonyan, L. Sifre, L. Martens, M. Johnson,  \nM. O’Neill, N. Antropova, R. Hadsell, S. Blackwell, S. Das, S. Hou, S. Gouws, S. Wheelwright,  \nT. Hennigan, T. Ward, Z. Wu, Ž. Avsec and the Research Platform Team for their contributions; \nM. Mirdita for his help with the datasets; M. Piovesan-Forster, A. Nelson and R. Kemp for their \nhelp managing the project; the JAX, TensorFlow and XLA teams for detailed support and \nenabling machine learning models of the complexity of AlphaFold; our colleagues at \nDeepMind, Google and Alphabet for their encouragement and support; and J. Moult and  \nthe CASP14 organizers, and the experimentalists whose structures enabled the assessment.  \nM.S. acknowledges support from the National Research Foundation of Korea grant \n(2019R1A6A1A10073437, 2020M3A9G7103933) and the Creative-Pioneering Researchers \nProgram through Seoul National University.\nAuthor contributions J.J. and D.H. led the research. J.J., R.E., A. Pritzel, M.F., O.R., R.B.,  \nA. Potapenko, S.A.A.K., B.R.-P., J.A., M.P., T. Berghammer and O.V. developed the neural network  \narchitecture and training. T.G., A.Ž., K.T., R.B., A.B., R.E., A.J.B., A.C., S.N., R.J., D.R., M.Z. and S.B. \ndeveloped the data, analytics and inference systems. D.H., K.K., P.K., C.M. and E.C. managed \nthe research. T.G. led the technical platform. P.K., A.W.S., K.K., O.V., D.S., S.P. and T. Back \ncontributed technical advice and ideas. M.S. created the BFD genomics database and \nprovided technical assistance on HHBlits. D.H., R.E., A.W.S. and K.K. conceived the AlphaFold \nproject. J.J., R.E. and A.W.S. conceived the end-to-end approach. J.J., A. Pritzel, O.R.,  \nA. Potapenko, R.E., M.F., T.G., K.T., C.M. and D.H. wrote the paper.\nCompeting interests J.J., R.E., A. Pritzel, T.G., M.F., O.R., R.B., A.B., S.A.A.K., D.R. and A.W.S. have \nfiled non-provisional patent applications 16/701,070 and PCT/EP2020/084238, and provisional \npatent applications 63/107,362, 63/118,917, 63/118,918, 63/118,921 and 63/118,919, each in the \nname of DeepMind Technologies Limited, each pending, relating to machine learning for \npredicting protein structures. The other authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary material available at \nhttps://doi.org/10.1038/s41586-021-03819-2.\nCorrespondence and requests for materials should be addressed to J.J. or D.H.\nPeer review information Nature thanks Mohammed AlQuraishi, Charlotte Deane and Yang \nZhang for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n1\nnature research  |  reporting summary\nApril 2020\nCorresponding author(s):\nJohn Jumper, Demis Hassabis\nLast updated by author(s): Jul 11, 2021\nReporting Summary\nNature Research wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \nin reporting. For further information on Nature Research policies, see our Editorial Policies and the Editorial Policy Checklist.\nStatistics\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\nn/a Confirmed\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\nThe statistical test(s) used AND whether they are one- or two-sided \nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\nA description of all covariates tested\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) \nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \nGive P values as exact values whenever suitable.\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\nOur web collection on statistics for biologists contains articles on many of the points above.\nSoftware and code\nPolicy information about availability of computer code\nData collection\nSource code for the AlphaFold model, trained weights, and inference script will be made available under an open-source license at https://\ngithub.com/deepmind/ upon publication. \n \nNeural networks were developed with TensorFlow v1 (https://github.com/tensorflow/tensorflow), Sonnet v1 (https://github.com/deepmind/\nsonnet), JAX v0.1.69 (https://github.com/google/jax/), and Haiku v0.0.4 (https://github.com/deepmind/dm-haiku). The XLA compiler is \nbundled with JAX and does not have a separate version number. \n \nFor MSA search on BFD+Uniclust30, and for template search against PDB70, we used HHBlits and HHSearch from hh-suite v3.0-beta.3 \n14/07/2017 (https://github.com/soedinglab/hh-suite). For MSA search on UniRef90 and clustered MGnify, we used jackhmmer from HMMER \nv3.3 (http://eddylab.org/software/hmmer/). For constrained relaxation of structures, we used OpenMM v7.3.1 (https://github.com/openmm/\nopenmm) with the Amber99sb force field.  \n \nConstruction of BFD used MMseqs2 version 925AF (https://github.com/soedinglab/MMseqs2) and FAMSA v1.2.5 (https://github.com/refresh-\nbio/FAMSA). \nData analysis\nData analysis used Python v3.6 (https://www.python.org/), NumPy v1.16.4 (https://github.com/numpy/numpy), SciPy v1.2.1 (https://\nwww.scipy.org/), seaborn v0.11.1 (https://github.com/mwaskom/seaborn), Matplotlib v3.3.4 (https://github.com/matplotlib/matplotlib), \nbokeh v1.4.0 (https://github.com/bokeh/bokeh), pandas v1.1.5 (https://github.com/pandas-dev/pandas), plotnine v0.8.0 (https://github.com/\nhas2k1/plotnine), statsmodels v0.12.2 (https://github.com/statsmodels/statsmodels) and Colab (https://research.google.com/colaboratory). \nTM-align v20190822 (https://zhanglab.dcmb.med.umich.edu/TM-align/) was used for computing TM-scores. Structure visualizations were \ncreated in Pymol v2.3.0 (https://github.com/schrodinger/pymol-open-source).\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and \nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code & software for further information.\n2\nnature research  |  reporting summary\nApril 2020\nData\nPolicy information about availability of data\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \n- Accession codes, unique identifiers, or web links for publicly available datasets \n- A list of figures that have associated raw data \n- A description of any restrictions on data availability\nAll input data are freely available from public sources. \n \nStructures from the PDB were used for training and as templates (https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated sequence data and 40% sequence \nclustering see also https://ftp.wwpdb.org/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/sequence/clusters/bc-40.out). Training used a version of the \nPDB downloaded 28/08/2019, while CASP14 template search used a version downloaded 14/05/2020. Template search also used the PDB70 database, downloaded \n13/05/2020 (https://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/). \n \nWe show experimental structures from the PDB with accessions 6Y4F77, 6YJ178, 6VR479, 6SK080, 6FES81, 6W6W82, 6T1Z83, and 7JTL84. \n \nFor MSA lookup at both training and prediction time, we used UniRef90 v2020_01 (https://ftp.ebi.ac.uk/pub/databases/uniprot/previous_releases/\nrelease-2020_01/uniref/), BFD (https://bfd.mmseqs.com), Uniclust30 v2018_08 (https://wwwuser.gwdg.de/~compbiol/uniclust/2018_08/), and MGnify clusters \nv2018_12 (https://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2018_12/).  \nUniclust30 v2018_08 was further used as input for constructing a distillation structure dataset.\nField-specific reporting\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\nLife sciences\nBehavioural & social sciences\n Ecological, evolutionary & environmental sciences\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\nLife sciences study design\nAll studies must disclose on these points even when the disclosure is negative.\nSample size\nNo sample size was chosen; the method was evaluated on the full CASP14 benchmark set, and all PDB chains not in the training set (subject to \nthe exclusions noted below). \nData exclusions\nThe recent PDB set was filtered (see Methods for full details). Briefly this excludes chains with too few resolved residues, longer than 1400 \nresidues, solved by NMR or with unknown/ambiguous residues. This set was also redundancy reduced (by taking representatives from a \nsequence clustering), and for some figures a sequence similarity-based filter was applied to remove entries too similar to the training set (see \nMethods and figure legends for details).\nReplication\nNot applicable, no experimental work is described in this study. The results are the output of a computational method which will be made \navailable.\nRandomization\nNot applicable, we are not making a comparison between two groups\nBlinding\nNot applicable, we are not making a comparison between two groups\nReporting for specific materials, systems and methods\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \nMaterials & experimental systems\nn/a Involved in the study\nAntibodies\nEukaryotic cell lines\nPalaeontology and archaeology\nAnimals and other organisms\nHuman research participants\nClinical data\nDual use research of concern\nMethods\nn/a Involved in the study\nChIP-seq\nFlow cytometry\nMRI-based neuroimaging\n",
  "conversation": [
    {
      "user": "remote",
      "prompt": "We need to perform the following task.\n\n### Task\nExplain the deep learning architecture used?\n\n### Instructions\nYou will not have direct access to the context, but can chat with a small language model which has read the entire thing.\n\nFeel free to think step-by-step, but eventually you must provide an output in the format below:\n\n```json\n{\n    \"message\": \"<your message to the small language model. If you are asking model to do a task, make sure it is a single task!>\"\n}\n```\n",
      "output": "\n\n\n\n{\"message\": \"Can you describe the deep learning architecture used in the context you have read?\"}"
    },
    {
      "user": "local",
      "prompt": "Can you describe the deep learning architecture used in the context you have read?",
      "output": "The deep learning architecture used in this context is called AlphaFold, which is a neural network-based protein structure prediction model. The architecture consists of two main stages: the Evoformer block and the structure module.\n\n**Evoformer Block**\n\nThe Evoformer block is a novel neural network block that processes the input sequence through repeated layers of attention-based and non-attention-based components. The key innovations in this section are:\n\n1. **Multi-sequence alignment (MSA)**: The MSA representation is initialized with the raw MSA, but it's updated within every Evoformer block using an element-wise outer product that sums over the MSA sequence dimension.\n2. **Pair representation**: The pair representation encodes information about the relation between residues in proximity. It uses a novel update operation called \"triangle multiplicative update\" to update the missing third edge of a triangle involving three different nodes.\n3. **Attention mechanisms**: The Evoformer block uses attention-based and non-attention-based components, including axial attention and triangle self-attention.\n\n**Structure Module**\n\nThe structure module operates on a concrete 3D backbone structure using the pair representation and the original sequence row (single representation) of the MSA representation from the trunk. The key innovations in this section are:\n\n1. **Invariant point attention (IPA)**: The IPA module combines the pair representation, single representation, and geometric representation to update the single representation.\n2. **Rotation and translation**: The structure module introduces an explicit 3D structure in the form of a rotation and translation for each residue of the protein.\n3. **Equivariant transformer**: The network uses an equivariant transformer to allow it to implicitly reason about the unrepresented side-chain atoms.\n\n**Overall Architecture**\n\nThe AlphaFold architecture consists of two main stages: the Evoformer block and the structure module. The Evoformer block processes the input sequence through repeated layers of attention-based and non-attention-based components, while the structure module operates on a concrete 3D backbone structure using the pair representation and single representation.\n\n**Training Regimen**\n\nThe AlphaFold model is trained using a combination of objectives, including:\n\n1. **Frame-aligned point error (FAPE)**: The final loss function that compares the predicted atom positions to the true positions under many different alignments.\n2. **Auxiliary losses**: Additional losses, such as random masking on the input MSAs and predicting binned per-residue lDDT-Cα values.\n\nThe model is trained using a large dataset of protein structures from the PDB, with a maximum release date of 30 April 2018. The training regimen involves sampling chains in inverse proportion to cluster size, randomly cropping them to 256 residues, and assembling into batches of size 128."
    },
    {
      "user": "remote",
      "prompt": "\nHere is the response from the small language model:\n\n### Response\nThe deep learning architecture used in this context is called AlphaFold, which is a neural network-based protein structure prediction model. The architecture consists of two main stages: the Evoformer block and the structure module.\n\n**Evoformer Block**\n\nThe Evoformer block is a novel neural network block that processes the input sequence through repeated layers of attention-based and non-attention-based components. The key innovations in this section are:\n\n1. **Multi-sequence alignment (MSA)**: The MSA representation is initialized with the raw MSA, but it's updated within every Evoformer block using an element-wise outer product that sums over the MSA sequence dimension.\n2. **Pair representation**: The pair representation encodes information about the relation between residues in proximity. It uses a novel update operation called \"triangle multiplicative update\" to update the missing third edge of a triangle involving three different nodes.\n3. **Attention mechanisms**: The Evoformer block uses attention-based and non-attention-based components, including axial attention and triangle self-attention.\n\n**Structure Module**\n\nThe structure module operates on a concrete 3D backbone structure using the pair representation and the original sequence row (single representation) of the MSA representation from the trunk. The key innovations in this section are:\n\n1. **Invariant point attention (IPA)**: The IPA module combines the pair representation, single representation, and geometric representation to update the single representation.\n2. **Rotation and translation**: The structure module introduces an explicit 3D structure in the form of a rotation and translation for each residue of the protein.\n3. **Equivariant transformer**: The network uses an equivariant transformer to allow it to implicitly reason about the unrepresented side-chain atoms.\n\n**Overall Architecture**\n\nThe AlphaFold architecture consists of two main stages: the Evoformer block and the structure module. The Evoformer block processes the input sequence through repeated layers of attention-based and non-attention-based components, while the structure module operates on a concrete 3D backbone structure using the pair representation and single representation.\n\n**Training Regimen**\n\nThe AlphaFold model is trained using a combination of objectives, including:\n\n1. **Frame-aligned point error (FAPE)**: The final loss function that compares the predicted atom positions to the true positions under many different alignments.\n2. **Auxiliary losses**: Additional losses, such as random masking on the input MSAs and predicting binned per-residue lDDT-Cα values.\n\nThe model is trained using a large dataset of protein structures from the PDB, with a maximum release date of 30 April 2018. The training regimen involves sampling chains in inverse proportion to cluster size, randomly cropping them to 256 residues, and assembling into batches of size 128.\n\n\n### Instructions\nAnalyze the response and think-step-by-step to determine if you have enough information to answer the question.\n\nThink about:\n1. What information we have gathered\n2. Whether it is sufficient to answer the question\n3. If not sufficient, what specific information is missing\n4. If sufficient, how we would calculate or derive the answer\n\n",
      "output": "To determine if we have enough information to explain the deep learning architecture used, let's analyze the response step-by-step:\n\n1. **Information Gathered:**\n   - The architecture is AlphaFold, a neural network-based protein structure prediction model.\n   - It consists of two main stages: the Evoformer block and the structure module.\n   - **Evoformer Block:**\n     - Utilizes multi-sequence alignment (MSA) and pair representation.\n     - Employs attention mechanisms like axial attention and triangle self-attention.\n   - **Structure Module:**\n     - Uses invariant point attention (IPA) and introduces rotation and translation for residues.\n     - Incorporates an equivariant transformer for reasoning about unrepresented side-chain atoms.\n   - **Overall Architecture:**\n     - Describes the flow from Evoformer to the structure module.\n   - **Training Regimen:**\n     - Describes the loss functions and dataset used for training.\n\n2. **Sufficiency of Information:**\n   - The response provides a comprehensive overview of the AlphaFold architecture, detailing its components, mechanisms, and training regimen.\n   - It explains the key innovations and how the architecture processes input sequences and predicts protein structures.\n\n3. **Missing Information:**\n   - The response does not seem to lack any critical information needed to explain the architecture. It covers the main components, their functions, and the training process.\n\n4. **Conclusion:**\n   - The information provided is sufficient to explain the deep learning architecture used in AlphaFold. We can describe the architecture by summarizing the Evoformer block, structure module, and training regimen as outlined in the response.\n\nSince we have enough information, we can proceed to provide the explanation based on the gathered details."
    },
    {
      "user": "remote",
      "prompt": "Here is the response after step-by-step thinking.\n\n### Response\nTo determine if we have enough information to explain the deep learning architecture used, let's analyze the response step-by-step:\n\n1. **Information Gathered:**\n   - The architecture is AlphaFold, a neural network-based protein structure prediction model.\n   - It consists of two main stages: the Evoformer block and the structure module.\n   - **Evoformer Block:**\n     - Utilizes multi-sequence alignment (MSA) and pair representation.\n     - Employs attention mechanisms like axial attention and triangle self-attention.\n   - **Structure Module:**\n     - Uses invariant point attention (IPA) and introduces rotation and translation for residues.\n     - Incorporates an equivariant transformer for reasoning about unrepresented side-chain atoms.\n   - **Overall Architecture:**\n     - Describes the flow from Evoformer to the structure module.\n   - **Training Regimen:**\n     - Describes the loss functions and dataset used for training.\n\n2. **Sufficiency of Information:**\n   - The response provides a comprehensive overview of the AlphaFold architecture, detailing its components, mechanisms, and training regimen.\n   - It explains the key innovations and how the architecture processes input sequences and predicts protein structures.\n\n3. **Missing Information:**\n   - The response does not seem to lack any critical information needed to explain the architecture. It covers the main components, their functions, and the training process.\n\n4. **Conclusion:**\n   - The information provided is sufficient to explain the deep learning architecture used in AlphaFold. We can describe the architecture by summarizing the Evoformer block, structure module, and training regimen as outlined in the response.\n\nSince we have enough information, we can proceed to provide the explanation based on the gathered details.\n\n### Instructions\nIf you have enough information or if the task is complete, write a final answer to fullfills the task. \n\n```json\n{\n    \"decision\": \"provide_final_answer\", \n    \"answer\": \"<your answer>\"\n}\n```\n\nOtherwise, if the task is not complete, request the small language model to do additional work, by outputting the following:\n\n```json\n{\n    \"decision\": \"request_additional_info\",\n    \"message\": \"<your message to the small language model>\"\n}\n```\n\n",
      "output": "\n{\n    \"decision\": \"provide_final_answer\",\n    \"answer\": \"The deep learning architecture used is AlphaFold, a neural network-based model for protein structure prediction. It consists of two main stages: the Evoformer block and the structure module. The Evoformer block processes input sequences using multi-sequence alignment (MSA) and pair representation, employing attention mechanisms like axial attention and triangle self-attention. The structure module uses invariant point attention (IPA) and introduces rotation and translation for residues, incorporating an equivariant transformer to reason about unrepresented side-chain atoms. The model is trained using a combination of objectives, including frame-aligned point error (FAPE) and auxiliary losses, on a large dataset of protein structures. This comprehensive architecture allows AlphaFold to predict protein structures with high accuracy.\"\n}"
    }
  ],
  "generated_final_answer": "The deep learning architecture used is AlphaFold, a neural network-based model for protein structure prediction. It consists of two main stages: the Evoformer block and the structure module. The Evoformer block processes input sequences using multi-sequence alignment (MSA) and pair representation, employing attention mechanisms like axial attention and triangle self-attention. The structure module uses invariant point attention (IPA) and introduces rotation and translation for residues, incorporating an equivariant transformer to reason about unrepresented side-chain atoms. The model is trained using a combination of objectives, including frame-aligned point error (FAPE) and auxiliary losses, on a large dataset of protein structures. This comprehensive architecture allows AlphaFold to predict protein structures with high accuracy."
}